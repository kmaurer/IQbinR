### Helper functions constructed for comparison studies
# (not included in iqbin package due to limited generalizability)

#--------------------------------------------------------------------------------------
### Timing Study helper function
#--------------------------------------------------------------------------------------

# Predictions with timer for AKNN methods using FNN package functions
aknn_predict <- function(train_x,test_x,y,mod_type="class",k=10,algorithm="cover_tree"){
  # find neighbors (and time it)
  timer <- Sys.time()
  neighbors <- get.knnx(data=train_x, query=test_x, k=k, algorithm=algorithm)$nn.index
  fittime <- as.numeric(Sys.time() - timer,units="mins")
  # overwrite any negative indeces generated by get.knnx
  if(sum(as.vector(neighbors<=0),na.rm=T) > 0){
    for(col in 1:ncol(neighbors)){
      neighbors[which(neighbors[,col]<=0),col] <- NA
    }
  }
  # make predictions (and time it)
  if(mod_type=="class"){
    timer <- Sys.time()
    preds <- sapply(1:nrow(test_x), function(x) {
      iqbin::majority_vote(as.character(y[neighbors[x,]]))
    })
    predtime <- as.numeric(Sys.time() - timer,units="mins")
  } else if(mod_type=="reg") {
    timer <- Sys.time()
    preds <- sapply(1:nrow(test_x), function(x) {
      mean(y[neighbors[x,]], na.rm=TRUE) 
    })
    predtime <- as.numeric(Sys.time() - timer,units="mins")
  } else {
    print("Must specify mod_type as either class or reg")
    preds <- NULL; fittime <- NULL;predtime <- NULL
  }
  return(list(preds=preds,fittime=fittime,predtime=predtime))
}

#--------------------------------------------------------------------------------------
### Accuracy comparison helper functions
#--------------------------------------------------------------------------------------

#function to standardize and drop non-numeric/constant-valued input variables
clean_data_for_iqnn_knn <- function(data,y){
  data <- as.data.frame(data)
  # Fix accidental spaces before some column names
  names(data) <- stringr::str_replace_all(names(data), " ", "")  
  names(data) <- stringr::str_replace_all(names(data), "\\.", "")
  names(data) <- stringr::str_replace_all(names(data), "-", "_")
  names(data) <- stringr::str_replace_all(names(data), "\\(", "_")
  names(data) <- stringr::str_replace_all(names(data), "\\)", "_")
  # keep only numeric input columns with at sqrt(n)/10 least unique values
  keeper_cols <- sapply(data, function(x) is.numeric(x) & (length(unique(x))>=sqrt(length(x))/10) )
  keeper_cols[which(names(data)==y)] <- TRUE
  data <- data[,keeper_cols]
  # Drop Rows with Missing Values
  data <- na.omit(data)
  # Convert to standardized values for each input variable
  data[,(names(data)!=y)] <- sapply(data[,(names(data)!=y)], function(x) as.numeric(scale(x)))
  # Drop columns with Missing Values (ie columns with no variability that lead to NA's when the scale function divided by sd=0)
  data <- data[,sapply(data, function(x) !all(duplicated(x)[-1L]))]
  return(data)
}


# Tuning function for knn classifier
tune_knn_class <- function(data, y_name, x_names, cv_method="kfold", cv_k = 10, k_values=NULL, knn_algorithm = "brute"){
  if(!is.integer(k_values)) return(print("Please specify k_values as an integer vector of neightborhood sizes (k) to be tuned"))
  cv_results <- data.frame(k=k_values,error = NA)
  for(k_idx in 1:length(k_values)){
    cv_preds <- cv_pred_knn_class(data, y_name, x_names, cv_method="kfold", cv_k = cv_k, k=k_values[k_idx], knn_algorithm = "brute")
    cv_results$error[k_idx] <- sum(cv_preds!=data[,y_name]) / nrow(data)
  }
  return(cv_results)
}

# Tuning function for knn regression
tune_knn_reg <- function(data, y_name, x_names, cv_k = 10, k_values=NULL, knn_algorithm = "brute"){
  if(!is.integer(k_values)) return(print("Please specify k_values as an integer vector of neightborhood sizes (k) to be tuned"))
  cv_results <- data.frame(k=k_values,MSE = NA)
  for(k_idx in 1:length(k_values)){
    cv_preds <- cv_pred_knn(data, y_name, x_names, cv_k = cv_k, k=k_values[k_idx], knn_algorithm = "brute")
    cv_results$MSE[k_idx] <- mean((data[,y_name]-cv_preds)^2)
  }
  cv_results$RMSE <- sqrt(cv_results$MSE)
  return(cv_results)
}

# knn_cv_predict with output as list containing predictions AND timing values for fitting and predicting
knn_cv_pred_timer <- function(data, y, x_names, cv_k = 10, k=5, knn_algorithm = "brute", seed=sample(1:100000, 1)){
  data <- as.data.frame(data)
  set.seed(seed)
  cv_cohorts <- make_cv_cohorts(data, cv_k)
  cv_preds <- factor(rep("NA",nrow(data)),levels=levels(data[,y]))
  time_knn <- 0
  for(fold in 1:length(unique(cv_cohorts))){
    test_index <- which(cv_cohorts==fold)
    timer <- Sys.time()
    knn_mod <- knn(train=data[-test_index,x_names], test=data[test_index,x_names], 
                   cl=data[-test_index,y], k = k, algorithm = knn_algorithm)
    time_knn <- time_knn + difftime(Sys.time(),timer,units="secs")
    cv_preds[test_index] <- knn_mod
  }
  timer <- Sys.time()
  knn_mod <- knn(train=data[-test_index,x_names], test=data[1,x_names], 
                 cl=data[-test_index,y], k = k, algorithm = knn_algorithm) 
  time_fit <- cv_k * difftime(Sys.time(),timer,units="secs")
  
  return(list(preds=cv_preds,time_fit=time_fit, pred_time=time_knn-time_fit))
}

### Cross Validated predictions for knn model using knn.reg from FNN package
cv_pred_knn <- function(data, y_name, x_names, cv_k = 10, k=5, knn_algorithm = "brute"){
  data <- as.data.frame(data)
  cv_cohorts <- make_cv_cohorts(data, cv_k)
  cv_preds <- rep(NA,nrow(data))
  for(fold in 1:length(unique(cv_cohorts))){
    test_index <- cv_cohorts==fold
    knn_mod <- knn.reg(train=data[!test_index,x_names], test=data[test_index,x_names], 
                       y=as.vector(data[!test_index,y_name]), k = k, algorithm = knn_algorithm)
    cv_preds[test_index] <- knn_mod$pred
  }
  cv_preds
}