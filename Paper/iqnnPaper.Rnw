\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,psfrag,epsf,float,wrapfig,subfig,tabularx,ulem}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{algorithm2e}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

 \usepackage[utf8]{inputenc}
 \usepackage{fullpage}
 %\graphicspath{{figure/}}
 %\usepackage{csquotes}
 \usepackage{color}
 \usepackage{hyperref}
 \usepackage{mathrsfs}
 
\newcommand{\blind}{0}
\newcommand{\ktm}[1]{{\color{red} #1}} %red comments: 

<<setup,echo=F,include=F,eval=T>>=
options(scipen=5, digits=5)

# libraries
library(iqbin)
library(mvtnorm)
library(tidyverse)

# source in functions

# Load all data

@

%-----------------------------------------------------------------------------------------------------
%%%% Document
%-----------------------------------------------------------------------------------------------------
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%-----------------------------------------------------------------------------------------------------
%%% Title
%-----------------------------------------------------------------------------------------------------
\if0\blind
{
%opening
\title{Iterative-Quantile Nearest-Neighbors}
\author{author names}
  \maketitle
} \fi

\if1\blind
{
\title{Iterative-Quantile Nearest-Neighbors}
\author{Author A$^1$, Author B$^{2,3}$, Author C$^2$\\$^1$Affiliation X \\$^2$Affiliation Y \\$^3$Affiliation Z}
  \maketitle
} \fi

%-----------------------------------------------------------------------------------------------------
%%% Abstract
%-----------------------------------------------------------------------------------------------------
\bigskip
\begin{abstract}
Write abstract here 
\end{abstract}

\noindent%
{\it Keywords:} Non-parametric, Predictive Modeling, Partitioning, knn, R-Trees
\vfill

%-----------------------------------------------------------------------------------------------------
% Intro
%-----------------------------------------------------------------------------------------------------
\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{Intro}

The nearest-neighbor search is a classic problem in computer science that has applications within many fields.\ktm{(example citations)}. Within the context of statistical learning the k-nearest neighbors search is used as a non-parametric method for gathering training observations used to predict for both regression and classifier models. The basic mechanism for $k$-nearest-neighbor (KNN) models is to use numeric feature variables to calculate distances between a new query observation and all the observations in the training data, then create a prediction based on the responses from the $k$ training observations with the smallest distances from the query point \ktm{(add knn citation)}; most typically using an average of response values for KNN regressions, and the majority vote for KNN classifiers \ktm{(general KNN learning citation)}. 

Given the rapid growth in data volume and availability, it is important to consider how the computational requirements of a statistical learning method scale with increase training and test data sizes. KNN models rely on calculation of $p$-dimensional distances - typically Euclidean - between the $n$ observations in the training set for each new query observation, which can be solved in O(n) time through a brute force approach. This can clearly become computationally demanding when either the training data size increases. The computational demands are compounded when using KNN models to make a large number of predictions. In a test set with $m$ observations, every new query requires recalculation of distances; solved by brute force in O(nm) time. 

Algorithms have been developed to improve the speed at which queries are able to run by pre-processing the training data to allow for more efficient nearest-neighbor searches using tree-based structures. Roussopoulos et al. presented the Branch-and-Bound R-tree method, using minimal bounding rectangles to group observations for efficient nearest-neighbor searches. While the method makes clever use of nested hyper-bins, it is fundamentally focused on finding the single nearest-neighbor and the computational gains erode quickly as query neighborhood size, $k$, increases \ktm{(Roussopoulos, Kelley and Vincent	1995)}. Arya and Mount presented another method for preprocessing the training data into a kd-tree structure for efficient \textit{approximate k-nearest-neighbor} (AKNN) queries; a set of $k$ observations with distances within a ratio of $(1+\epsilon)$ to the query point as the true $k^{th}$-nearest-neighbor, for some specified $\epsilon$. This kd-tree AKNN method requires $O(n^2)$ time to organize the data into nested conically shaped search regions that are then stored in $O(n\log(n))$ space, and AKNN queries then require only $O(\log^3(n))$ time \ktm{(Arya and Mount	1993)}. Arya et al. then optimized the AKNN algorithm to preprocess the kd-tree in $O(n\log(n))$ time and store in $O(n)$ space, then able to make AKNN queries in $O(c_{p\epsilon}\log(n))$ time, where $c_{p\epsilon}$ is a constant bounded above by a function of the dimensionality, $p$, and the approximation parameter, $\epsilon$ \ktm{(Arya, Mount, Netanyahu, Silverman and Wu	1998)}. Beygelzimer et al. provide the cover-tree method as an alternative for either KNN or AKNN searches. Cover-tree AKNN approach take $O(c^6 n\log(n))$ time to construct the cover-tree, required $O(n)$ storage space and allows nearest-neighbor queries in $O(c^{12} \log(n))$ time, where $c$ is a bounded expansion constant \ktm{(Beygelzimer, Kakade, Langford	2006)}.  

Instance selection methodology provides a fundamentally different option for speeding up KNN queries, aiming to select a small targeted subsample from the training data on which to compare new observations. By greatly reducing the train set size, $n$, the computational cost of using even a brute force KNN query may be acceptable. Popular instance selection algorithms include \ktm{... Walter suggestions?} 

\ktm{paragraph transitioning to discussions on predictive modeling goals of KNN classification/regression}

We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for classification and regression that provides a rough approximation to $k$-nearest-neighborhood models through the use of an iterative process of partitioning the feature space using empirical-quantiles of the variable from the training data. In Section~\ref{iqnn} we explain the intuition and procedural details of the IQNN algorithm. The method requires at most $O(pn\log(n))$ time to construct the nested quantile based partitions on $p$-dimensional features, which are then stored into an R-tree structure using at most $O(pn)$\ktm{???} space, and iterative-quantile neighborhoods queried in $O(\sum_{j=1}^{p}b_j)$ time, where $b_j$ is the number of partitions in the $j^{th}$ feature dimension. In Section \ktm{(Ref needed)} we evaluate the theoretically expected computational costs of the IQNN approach and then compare the timing performance and predictive accuracy of the IQNN models to the kd-tree AKNN , cover-tree AKNN, and \ktm{some instance selection methods} on a number of real data sets with both numeric and categorical response values. Finally, in Section \ktm{(Ref Needed)} we discuss the results of our computational testing and identify areas for future research.  

\section{Iterative Quantile Nearest-Neighbors}
\label{iqnn}

To lay out the proposed method for iterative quantile nearest-neighbor modeling we begin in Subsection~\ref{unibin} with the simple case of a univariate feature space, which requires only a single stage of binned partitioning based on empirical quantiles. Then in Subsection~\ref{iqbin} we explore how the partitioning is iteratively repeated for each subsequent variable in higher dimensional feature spaces. In Subsection~\ref{iqnnmodels} we formalize IQNN models for classification and regression for higher dimensional feature spaces.

\subsection{Univariate Quantile Binned Neighborhoods}
\label{unibin}

Partitioning a continuous range of numeric values using a set of adjacent intervals is commonly referred to as binning. For the univariate case with observations, $x_i \in \mathbb{R}$ for $i \in \{1,\dots,n\}$, binning algorithms employ a function $b_X(.) : x_i \rightarrow b_j$ that maps observations to unique bin indices $j \in \{1,\dots,J\}$. We define a defining a sequence of $J$ adjacent intervals, $(\beta_{j-1},\beta_{j}]$ for bins $j \in \{1,\dots,J\}$, and set the outermost bounds, $\beta_{0}=min(x_i)$ and $\beta_{J}=max(x_i)$. Half-open intervals are used such that any observation falling on a bin boundary is assigned to a unique interval and values $x_i$ exactly equal to the lowest bin boundary $\beta_0$ are grouped into the first bin to close the leftmost bound. This is expressed mathematically using the binning function $b_X(.) : x_i \rightarrow j$ defined as 
%
\begin{eqnarray}\label{rectbin}
b_X(x_i) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x_i = \beta_{0} \\
  j & \text{ for all } x_i \in (\beta_{j-1} , \beta_j] 
  \end{array}\right.
\end{eqnarray}  

It is clear that any new query point, $q \in \mathbb{R}$, can be allocated to a bin by comparing the value to the set of $J+1$ bin bounds. This binning is notably faster than calculating the distances from the training points if $J$ is much smaller than $n$. For a training data set containing a single continuous numeric feature variable and new query point $q\in\mathbb{R}$, we define the \textit{bin neighbors} as the set of training points $\bold{x}_q = \{x_i \hspace{.2cm} | \hspace{.2cm} b_X(x_i) = b_X(q) \}$. It is clear that the distance between a query point and any of its bin neighbors is bounded
\begin{center}
$d(x_i,q) = (x_i - q) < (\beta_j - \beta_{j-1})  \hspace{.2cm} \forall \hspace{.2cm} x_i \in \bold{x}_q$,
\end{center}
however the neighbors in each bin may vary depending on bin boundaries and the distribution of training points.Attempting to control $d(x_i,q)$ by setting regularly spaced intervals for any non-uniformly distributed training set would result in imbalanced bin counts; thus, a poor alternative to $k$-nearest neighborhoods. 

We define \textit{quantile binning} through specifying that the $j^{th}$ bin interval, $(\beta_{j-1} , \beta_j]$, takes the form $(Q_X(\frac{j-1}{J})\hspace{.2cm},\hspace{.2cm}Q_X(\frac{j}{J})]$, where $Q_X(p)$ is the the $p^{th}$ empirical quantile using the inverse empirical distribution function \ktm{\citep{hyndman1996sample}}. The quantile binning function $b_X^q(.) : x_i \rightarrow j$ defined as 
%
\begin{eqnarray}\label{rectbin}
b_X^q(x_i) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x_i = \min(x_i) \\
  j & \text{ for all } x_i \in (Q_X(\frac{j-1}{J}) \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{j}{J})] 
  \end{array}\right.
\end{eqnarray}   The resulting $J$ partitions will have interval widths that vary base on distribution, but will each contain either $\lceil \frac{n}{J} \rceil$ or $\lceil \frac{n}{J} \rceil - 1$ training points; thus roughly approximating a $(\frac{n}{J})$-nearest-neighborhood. It should be noted that we are assuming training points $x_i \in \mathbb{R}$ to be truly continuous in nature. If the training data is collected with poor measurement precision or have naturally discrete values, repeated values for $x_i$ could result in empirical quantile where $\beta_j = \beta_k$ for some $j \ne k$, thus non-unique bin assignments. In practice, the simple addition of a small amount of random noise (example: $Uniform(-\eta,\eta)$ , for some small $\eta > 0$) may be added to each value for discrete feature variables to allow for unique quantile bin bounds.

We can then roughly approximate KNN classification or regression modeling by using the quantile bin neighbor response values $\bold{y}_q = \{y_i \hspace{.2cm} | \hspace{.2cm} x_i \in \bold{x}_q\}$. The query point is given an estimated response, $\hat{y_q} = S(\bold{y}_q)$, for some summary statistic $S(.)$ that is appropriate for response type; an average for numeric responses or a majority vote count for categorical responses. The estimated responses for each bin may be pre-calculated and stored for quick retrieval in situations with a large number of query points; unlike KNN models where the estimated response needs to be calculated for each unique query point. 



\subsection{Iterative Quantile Binning}
\label{iqbin}

Now suppose we wish to create binned neighborhoods to approximate $k$-nearest neighborhoods within a $p$-dimensional continuous valued feature space, $\vec{x}_i \in \mathbb{R}^p$. A $p$-dimensional bin can be defined using a $p$-dimensional hyper-rectangle with interval boundaries running orthogonal to the coordinate axes. It would be simplest to generate a univariate binning function for each dimension then apply these independently to form a $p$-dimensional grid of bin boundaries. In many cases the dependence between the feature variables would result in $p$-dimensional bins with highly unbalanced counts, and thus would provide a poor substitute for the equal counts in $k$-nearest neighborhoods. Instead we propose the \textit{iterative-quantile binning} process: starting with the univariate quantile binning described in Subsection~\ref{unibin} above, then iteratively using univariate quantile-bin partitions \textit{nested within} each partition from the previous dimension. By iteratively subdividing nearly equal sized groups at each stage, the resulting $p$-dimensional bins will maintain bin neighborhood counts that are nearly balanced throughout all $p$-dimensions. 

<<2d_iqbin_plots, echo=F,include=T,eval=T,fig.width=3, fig.height=3.5, out.width='.32\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Process for constructing IQNN regression model using 4X4 binning structure. (Left) first partition the training set using quartiles of $X_1$, (Center) then further partition each group using quartiles of $X_2$, (Right) then apply average $Y$ in each bin as the IQNN regression estimates for future points falling within each respective bin.">>=
set.seed(12345)
nsim=160
simbins <- 4
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "bottom",
                     panel.border = element_blank())

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=.8) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

#iqbin_plot_2d(mybins) rework from here to allow closer match in progression of 3 plots
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=.8)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps, 
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+ 
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme
@

Figure~\ref{fig:2d_iqbin_plots} shows this process in the simple case where $\vec{x}_i \in \mathbb{R}^2$ and responses $y_i \in \mathbb{R}$. For this example we have randomly generated 160 feature points from a bivariate normal distribution with a strong positive covariance, and linearly related response values with additive random noise; parameterized such that 
\begin{center}
$\vec{x}_i \sim MVN\left(\begin{bmatrix}
  0 \\
  0
\end{bmatrix},\begin{bmatrix}
  1 & .5 \\
  .5 & 1
\end{bmatrix}\right)$, \\
\vspace{.4cm}
$y_i = x_{i1} + x_{i2} + \epsilon_i$, where $\epsilon_i \sim Normal(0,1)$
\end{center}
and iterative quantile binning with four bins per feature dimension is used to create iterative-quantile nearest-neighborhoods of size $k=10$. The left and center panels demonstrates the two stages of quantile-based partitioning necessary for this bivariate feature space. \\
%

\begin{algorithm}
\noindent \textbf{Specification:} Set order of features $\{X_1,X_2,...,X_p\}$ to match desired iterative binning order and number of bins $\{\delta_1,\delta_2,...,\delta_p\}$ for partitioning in each dimension 

\noindent \textbf{Binning:} \vspace{-.3cm}
  \begin{enumerate}
  \setlength\itemsep{0cm}
  \item Partition all points based on $\delta_1$ quantile bins over feature $X_1$ into index sets\\
  $\{B_1,...B_{\delta_1}\}$ such that $B_\ell = \{i \hspace{.2cm} | \hspace{.2cm} b_{X_1}^q(x_{i1}) = \ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...\delta_1$
  \item Repeat the following for $j = 2,...,p$ :
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
    \item Subdivide each $B_\ell$ using  $\delta_j$ quantile bins over feature $X_j$ to create new index sets
    $C_{m n} = \{i \hspace{.2cm} | \hspace{.2cm} i \in B_m$ and $b_{X_{j}}^q(x_{ij}) = n \} \hspace{.2cm} \forall \hspace{.2cm} m = 1,...,\displaystyle\prod_{d=1}^{j-1}\delta_d$ and $n = 1,...,\delta_j$
    \item Redefine index sets $\{B_1,...B_L\}$ such that $B_\ell = C_{m n}$, where $\ell = n(m-1) + n$ 
    \end{enumerate}
    \vspace{-.3cm}
\end{enumerate}
  
\noindent \textbf{Outputs:}
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
  \item Bin neighbor sets $\vec{\bold{x}}_\ell = \{\vec{x_i} \hspace{.1cm} | \hspace{.1cm}  i \in B_\ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,L=\displaystyle\prod_{d=1}^{p}\delta_d$
    \item Hyper-rectangles for bins $\ell = 1,...,L$ as intervals $(\beta_{j\ell1} \hspace{.1cm} , \hspace{.1cm} \beta_{j\ell2}]$ for dimensions $j=1,...,p$ 
    \end{enumerate}


\caption{$p$-dimensional iterative-quantile binning}
\label{alg:iqalgorithm}
\end{algorithm}

This iterative quantile process is generalized for binning a set of $p$-dimensional points $[x_{i1},x_{i2},...,x_{ip}]^t = \vec{x}_i \in \mathbb{R}^p \hspace{.2cm} \forall \hspace{.2cm} i \in \{1,...,n\}$ in Algorithm~\ref{alg:iqalgorithm} The iterative quantile binning process generates $p$-dimensional hyper-rectangles in the feature space in $\mathbb{R}^p$ through nested univariate partitions which are stored as an R-tree structured list of depth $p$. The interval bounds for feature $X_{j-1}$ are act as a parent node branching down to the $\delta_j$ intervals for feature $X_j$. It is then simple to identify the iterative quantile bin containing a new query point $[q_1,...,q_p]^t = \vec{q} \in \mathbb{R}^p$ by finding the interval at the $j^{th}$ level of the R-tree containing $q_j$ then moving down the branches for each dimension. A benefit of this nested structure is that after identifying the interval in the $j^{th}$ level of the R-tree containing $j^{th}$ dimension of a query point, the branches below all other intervals at that level are ignored; thus allowing a query for membership among $\prod_{d=1}^{p}\delta_d$ unique $p$-dimensional bins to be completed by checking membership within at most $\sum_{d=1}^{p}\delta_d$ univariate intervals. We will let $b_{\bold{X}}^{iq}(.): \vec{q} \rightarrow \ell$ denote the function mapping query points to iterative quantile bins by passing through the branches of the R-tree of intervals. 



%-----------------------------------------------------------------------
\subsection{Iterative Quantile Nearest-Neighbor Modeling}
\label{iqnnmodels}

In Subsection~\ref{unibin} we used quantile bin neighborhoods to roughly approximate KNN regression and classification models. An extension to $p$-dimensional feature spaces can summary statistics of the training response values within iterative-quantile bins to be used as predicted responses for test points. For this proposed method we define \textit{iterative-quantile nearest-neighbor} (IQNN) models for regression and classification such that

\begin{center}
$\hat{y}_{\vec{q}} = \displaystyle\sum_{i\in B_{\vec{q}}}\vec{x}_i/|B_{\vec{q}}|$ 
\end{center}

the iterative-quantile binning provides index sets $B_\ell = \{i \hspace{.15cm} | \hspace{.15cm} \vec{x}_i \text{ in bin }\ell \} \hspace{.15cm} \forall \hspace{.15cm} \ell = 1,...,L$, 

\begin{itemize}
\item Use higher dimensional bin notation $\vec{\bold{x}}_q$ and $\bold{y}_q$ to specify IQNN regression 
\item Refer back to figure 1 right panel for example
\item Use higher dimensional bin notation $\vec{\bold{x}}_q$ and $\bold{y}_q$ to specify IQNN classification
\item Problem with extrapolation, stretching, open outer
\item most similar to knn if nbins nearly equal for all dimensions
\item no need to scale like knn but tuning requires more future work 
\end{itemize}

Iterative-quantile bins are constructed in a way that can provide a potential alternative to $k$-nearest neighborhoods within training data, but it is important to recognize the fundamental differences in behavior that results from parameterization and treatment of feature values. The KNN model has a much simpler paramerization, requiring only the neighborhood size $k$ to be set as some positive integer.   The magnitude of feature values in each dimension can hold major influence on $p$-dimensional distances, thus making the scaling and units associated with feature values very important in KNN. The groupings created by iterative-quantile neighborhoods are invariant to any \ktm{term for positive one-to-one?} scaling because the unchanged ordering of observations in each dimension leads to the same quantile-based partitions.







\newpage

\begin{table}[h]
\centering
\begin{tabular}{lll} \hline
 & Bin Boundaries & Bin Centers \\ 
 \hline  
General &  $ \{\beta_j \text{ }|\text{ } \beta_j > \beta_{j-1} \} $ & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = (\beta_{j-1}+ \beta_j)/2 \}$ \\
%-------------
Standard \hspace{0.5cm} & $ \{\beta_j \text{ }|\text{ } \beta_j = \beta_{j-1} + \omega_X \} $\hspace{0.5cm} & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = \beta_{j-1} + \omega_X/2 \}$ \\
%-------------
Quantile & $ \{\beta_j \text{ }|\text{ } \beta_j = Q_X(j/J) \} $  & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = Q_X((j-0.5)/J) \}$ \\
\hline
\end{tabular}
\caption{Rectangular Binning Specifications}
\label{tab:rectbinning}
\end{table}
%

%-----------------------------------------------------------------------------

\section{Section Title}

% example Figure~\ref{fig:UniformStripes} below.
% \begin{figure}[hbtp]
% \centering
%   \subfloat[Binned scatterplots]{\includegraphics[keepaspectratio=TRUE,width=.63\textwidth]{./figure/CoarseUnif.png}} 
%   \caption[Traditional and adapted scatterplots for games vs. strikeouts data.]{\label{fig:UniformStripes} \ktm{Binned scatterplots and spatial losses for various sized bins with coarse uniform data. Note the slight dip in loss at even integers, when bins are aligned with data resolution.}}
% \end{figure}


<<ExampleCodeChunk, echo=F,include=T,eval=T,fig.width=8, fig.height=4, out.width='.9\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Figure Caption Here.">>= 
hist(1:5)
@


%----------------------------------------------------------------------------
\newpage
\begin{appendix}
\section{Appendix for Origin Offset Proof}  
%----------------------------------------------------------------------------
%\subsection{}
\label{proof:offset}

Appendix stuff goes here

\end{appendix}

\bibliographystyle{asa}

\spacingset{1} % reference spacing

\bibliography{references}

\end{document}
