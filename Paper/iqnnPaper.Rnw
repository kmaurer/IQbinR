\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,psfrag,epsf,float,wrapfig,subfig,tabularx,ulem}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{algorithm2e}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

 \usepackage[utf8]{inputenc}
 \usepackage{fullpage}
 %\graphicspath{{figure/}}
 %\usepackage{csquotes}
 \usepackage{color}
 \usepackage{hyperref}
 \usepackage{mathrsfs}
 
\newcommand{\blind}{0}
\newcommand{\ktm}[1]{{\color{red} #1}} %red comments: 

<<setup,echo=F,include=F,eval=T>>=
# options(scipen=5, digits=5)

# libraries
library(iqbin)
library(FNN)
library(tidyverse)
library(stringr)
library(randomForest)
library(RANN)
library(mvtnorm)
library(gridExtra)

# source in functions

# Load all data
load(file="../sim_all.Rdata")
load(file="../sim_times_agg.Rdata")
@

%-----------------------------------------------------------------------------------------------------
%%%% Document
%-----------------------------------------------------------------------------------------------------
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%-----------------------------------------------------------------------------------------------------
%%% Title
%-----------------------------------------------------------------------------------------------------
\if0\blind
{
%opening
\title{Iterative-Quantile Nearest-Neighbors}
\author{author names}
  \maketitle
} \fi

\if1\blind
{
\title{Iterative-Quantile Nearest-Neighbors}
\author{Author A$^1$, Author B$^{2,3}$, Author C$^2$\\$^1$Affiliation X \\$^2$Affiliation Y \\$^3$Affiliation Z}
  \maketitle
} \fi

%-----------------------------------------------------------------------------------------------------
%%% Abstract
%-----------------------------------------------------------------------------------------------------
\bigskip
\begin{abstract}
Write abstract here 
\end{abstract}

\noindent%
{\it Keywords:} Non-parametric, Predictive Modeling, Partitioning, knn, R-Trees
\vfill

%-----------------------------------------------------------------------------------------------------
% Intro
%-----------------------------------------------------------------------------------------------------
\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{intro}

The nearest-neighbor search is a classic computational challenge in computer science that has applications within many fields \citep{bhatia2010survey}. Within the context of machine learning the k-nearest neighbors search is used as a non-parametric method for gathering training observations used for prediction in both regression and classifier models (\citealt{fix1951discriminatory},\citealt{cover1967nearest}). The basic mechanism for $k$-nearest-neighbor (KNN) models is to use numeric feature variables to calculate distances between a new query observation and all the observations in the training data, then create a prediction based on the responses from the $k$ training observations with the smallest distances from the query point \citep{kuncheva2004combining}; commonly using an average of response values for KNN regressions, and the majority vote for KNN classifiers (\citealt{friedman2001elements};\citealt{james2013introduction}). 

Given the rapid growth in data volume and availability, it is important to consider how the computational requirements of a statistical learning method scale with increase training and test data sizes. KNN models rely on calculation of $p$-dimensional distances - typically Euclidean - between the $n$ observations in the training set for each new query observation, which can be solved in $O(n)$ time through a brute force approach. Then the distances must be sorted by magnitude to allow the $k$-nearest points be selected, taking an additional $O(nlog(n))$ time with a quick-sort algorithm. This can clearly become computationally demanding when either the training data size increases. The computational demands are compounded when using KNN models to make a large number of predictions. In a test set with $m$ observations, every new query requires recalculation of distances; solved by brute force computation in $O(mn(1+log(n)))$ time. 

Algorithms have been developed to improve the speed at which queries are able to run by pre-processing the training data to allow for more efficient nearest-neighbor searches using tree-based structures. Roussopoulos et al. presented the Branch-and-Bound R-tree method, using minimal bounding rectangles to group observations for efficient nearest-neighbor searches. While the method makes clever use of nested hyper-bins, it is fundamentally focused on finding the single neareroussopoulos1995nearestst-neighbor and the computational gains erode quickly as query neighborhood size, $k$, increases \citep{roussopoulos1995nearest}. Arya and Mount presented another method for pre-processing the training data into a kd-tree structure for efficient \textit{approximate k-nearest-neighbor} (AKNN) queries; a set of $k$ observations with distances within a ratio of $(1+\epsilon)$ to the query point as the true $k^{th}$-nearest-neighbor, for some specified $\epsilon$. This kd-tree AKNN method requires $O(n^2)$ time to organize the data into nested conically shaped search regions that are then stored in $O(n\log(n))$ space, and AKNN queries then require only $O(\log^3(n))$ time \citep{arya1993approximate}. Arya et al. then optimized the AKNN algorithm to pre-process the kd-tree in $O(n\log(n))$ time and store in $O(n)$ space, then able to make AKNN queries in $O(c_{p\epsilon}\log(n))$ time, where $c_{p\epsilon}$ is a constant bounded above by a function of the dimensionality, $p$, and the approximation parameter, $\epsilon$ \citep{arya1998optimal}. Beygelzimer et al. provide the cover-tree method as an alternative for either KNN or AKNN searches. Cover-tree AKNN approach take $O(c^6 n\log(n))$ time to construct the cover-tree, required $O(n)$ storage space and allows nearest-neighbor queries in $O(c^{12} \log(n))$ time, where $c$ is a bounded expansion constant \citep{beygelzimer2006cover}.  
% 
% Instance selection methodology provides a fundamentally different option for speeding up KNN queries, aiming to select a small targeted subsample from the training data on which to compare new observations. By greatly reducing the train set size, $n$, the computational cost of using even a brute force KNN query may be acceptable. Popular instance selection algorithms include \ktm{... Walter suggestions?} 

\ktm{paragraph transitioning to discussions on predictive modeling goals of KNN classification/regression. Include discussion about how KNN is a voroni partitioning of the feature space, can we approximate this partitioning roughly but faster? \citep{aurenhammer1991voronoi}}

We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for classification and regression that provides a rough approximation to $k$-nearest-neighborhood models through the use of an iterative process of partitioning the feature space using empirical-quantiles of the variable from the training data. In Section~\ref{iqnn} we explain the intuition and procedural details of the IQNN algorithm. The method requires at most $O(pn\log(n))$ time to construct the nested quantile based partitions on $p$-dimensional features, which are then stored into an R-tree structure using at most $O(pn)$\ktm{???} space, and iterative-quantile neighborhoods queried in $O(\sum_{j=1}^{p}\delta_j)$ time, where $\delta_j$ is the number of partitions in the $j^{th}$ feature dimension. In Section~\ref{eval} we evaluate the theoretically expected computational costs of the IQNN approach and then compare the timing performance and predictive accuracy of the IQNN models to the KNN, kd-tree AKNN, and cover-tree AKNN on a number of simulated and real data sets with both numeric and categorical response values. Finally, in Section~\ref{discussion} we discuss the results of our computational testing and identify areas for future research.  

\section{Iterative Quantile Nearest-Neighbors}
\label{iqnn}

To lay out the proposed method for iterative quantile nearest-neighbor modeling we begin in Subsection~\ref{unibin} with the simple case of a univariate feature space, which requires only a single stage of binned partitioning based on empirical quantiles. Then in Subsection~\ref{iqbin} we explore how the partitioning is iteratively repeated for each subsequent variable in higher dimensional feature spaces. In Subsection~\ref{iqnnmodels} we formalize IQNN models for classification and regression for higher dimensional feature spaces.

\subsection{Univariate Quantile Binned Neighborhoods}
\label{unibin}

Partitioning a continuous range of numeric values using a set of adjacent intervals is commonly referred to as binning. For the univariate case with observations, $x \in \mathbb{R}$ for $i \in \{1,\dots,n\}$, binning algorithms employ a function $b_X(.) : x \rightarrow b_j$ that maps observations to unique bin indices $j \in \{1,\dots,J\}$. We define a defining a sequence of $J$ adjacent intervals, $(\beta_{j-1},\beta_{j}]$ for bins $j \in \{1,\dots,J\}$, and set the outermost bounds, $\beta_{0}=min(x_i)$ and $\beta_{J}=max(x_i)$. Half-open intervals are used such that any observation falling on a bin boundary is assigned to a unique interval and values $x_i$ exactly equal to the lowest bin boundary $\beta_0$ are grouped into the first bin to close the leftmost bound. This is expressed mathematically using the binning function $b_X(.) : x \rightarrow j$ defined as 
%
\begin{eqnarray}\label{rectbin}
b_X(x) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x = \beta_{0} \\
  j & \text{ for all } x \in (\beta_{j-1} , \beta_j] 
  \end{array}\right.
\end{eqnarray}  

It is clear that any new query point, $q \in \mathbb{R}$, can be allocated to a bin by comparing the value to the set of $J+1$ bin bounds. This binning is notably faster than calculating the distances from the training points if $J$ is much smaller than $n$. For cases with one continuous numeric feature with training data points $x_i \in\mathbb{R}$ for $i=1,...,n$ and new query point $q\in\mathbb{R}$, we define the \textit{bin neighbors} as the set of training points $\mathbf{x}_q = \{x_i \hspace{.2cm} | \hspace{.2cm} b_X(x_i) = b_X(q) \}$. It is clear that the distance between a query point and any of its bin neighbors is bounded
\begin{center}
$d(x_i,q) = (x_i - q) < (\beta_j - \beta_{j-1})  \hspace{.2cm} \forall \hspace{.2cm} x_i \in \mathbf{x}_q$,
\end{center}
however the neighbors in each bin may vary depending on bin boundaries and the distribution of training points.Attempting to control $d(x_i,q)$ by setting regularly spaced intervals for any non-uniformly distributed training set would result in imbalanced bin counts; thus, a poor alternative to $k$-nearest neighborhoods. 

We define \textit{quantile binning} through specifying that the $j^{th}$ bin interval, $(\beta_{j-1} , \beta_j]$, takes the form $(Q_X(\frac{j-1}{J})\hspace{.2cm},\hspace{.2cm}Q_X(\frac{j}{J})]$, where $Q_X(p)$ is the the $p^{th}$ empirical quantile using the inverse empirical distribution function \citep{hyndman1996sample}. The quantile binning function $b_X^q(.) : x_i \rightarrow j$ defined as 
%
\begin{eqnarray}\label{quantbin}
b_X^q(x_i) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x_i = \min(x_i) \\
  j & \text{ for all } x_i \in (Q_X(\frac{j-1}{J}) \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{j}{J})] 
  \end{array}\right.
\end{eqnarray}   The resulting $J$ partitions will have interval widths that vary base on distribution, but will each contain either $\lceil \frac{n}{J} \rceil$ or $\lceil \frac{n}{J} \rceil - 1$ training points; thus roughly approximating a $(\frac{n}{J})$-nearest-neighborhood. It should be noted that we are assuming training points $x_i \in \mathbb{R}$ to be truly continuous in nature. If the training data is collected with poor measurement precision or have naturally discrete values, repeated values for $x_i$ could result in empirical quantile where $\beta_j = \beta_k$ for some $j \ne k$, thus non-unique bin assignments. In practice, the simple addition of a small amount of random noise (example: $Uniform(-\eta,\eta)$ , for some small $\eta > 0$) may be added to each value for discrete feature variables to allow for unique quantile bin bounds.

We can then roughly approximate KNN classification or regression modeling by using the quantile bin neighbor response values $\mathbf{y}_q = \{y_i \hspace{.2cm} | \hspace{.2cm} x_i \in \mathbf{x}_q\}$. The query point is given an estimated response, $\hat{y_q} = S(\mathbf{y}_q)$, for some summary statistic $S(.)$ that is appropriate for response type; an average of numeric responses or a majority vote count of categorical responses. The estimated responses for each bin may be pre-calculated and stored for quick retrieval in situations with a large number of query points; unlike KNN models where the estimated response needs to be calculated for each unique query point. 



\subsection{Iterative Quantile Binning}
\label{iqbin}

Now suppose we wish to create binned neighborhoods to approximate $k$-nearest neighborhoods within a $p$-dimensional continuous valued feature space, $\vec{x}_i \in \mathbb{R}^p$. A $p$-dimensional bin can be defined using a $p$-dimensional hyper-rectangle with interval boundaries running orthogonal to the coordinate axes. It would be simplest to generate a univariate binning function for each dimension then apply these independently to form a $p$-dimensional grid of bin boundaries. In many cases the dependence between the feature variables would result in $p$-dimensional bins with highly unbalanced counts, and thus would provide a poor substitute for the equal counts in $k$-nearest neighborhoods. Instead we propose the \textit{iterative-quantile binning} process: starting with the univariate quantile binning described in Subsection~\ref{unibin} above, then iteratively using univariate quantile-bin partitions \textit{nested within} each partition from the previous dimension. By iteratively subdividing nearly equal sized groups at each stage, the resulting $p$-dimensional bins will maintain bin neighborhood counts that are nearly balanced throughout all $p$-dimensions. 

<<2d_iqbin_plots, echo=F,include=T,eval=T,fig.width=3, fig.height=3.5, out.width='.32\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Process for constructing IQNN regression model using 3X3 binning structure. (Left) first partition the training set using tertiles of $X_1$, (Center) then further partition each group using tertiles of $X_2$, (Right) then apply average $Y$ in each bin as the IQNN regression estimates for future points falling within each respective bin.">>=
set.seed(12345)
nsim=90
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "bottom",
                     panel.border = element_blank())

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=.8) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

#iqbin_plot_2d(mybins) rework from here to allow closer match in progression of 3 plots
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=.8)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps, 
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+ 
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme
@

Figure~\ref{fig:2d_iqbin_plots} shows this process in the simple case where $\vec{x}_i \in \mathbb{R}^2$ and responses $y_i \in \mathbb{R}$. For this example we have randomly generated \Sexpr{nsim} feature points from a bivariate normal distribution with a strong positive covariance, and linearly related response values with additive random noise; parameterized such that 
\begin{center}
$\vec{x}_i \sim MVN\left(\begin{bmatrix}
  0 \\
  0
\end{bmatrix},\begin{bmatrix}
  1 & .5 \\
  .5 & 1
\end{bmatrix}\right)$, \\
\vspace{.4cm}
$y_i = x_{i1} + x_{i2} + \epsilon_i$, where $\epsilon_i \sim Normal(0,1)$
\end{center}
and iterative quantile binning with three bins per feature dimension is used to create iterative-quantile nearest-neighborhoods of size $k=10$. The left and center panels demonstrates the two stages of quantile-based partitioning necessary for this bivariate feature space. 

This iterative quantile-based process is generalized for binning a set of $p$-dimensional points $[x_{i1},x_{i2},...,x_{ip}]^t = \vec{x}_i \in \mathbb{R}^p \hspace{.2cm} \forall \hspace{.2cm} i \in \{1,...,n\}$ in Algorithm~\ref{alg:iqalgorithm} below.


\RestyleAlgo{boxruled}
\begin{algorithm}
\noindent \textbf{Specification:} Define order of features $\{X_1,X_2,...,X_p\}$ to match desired iterative binning order and number of bins $\{\delta_1,\delta_2,...,\delta_p\}$ for partitioning in each dimension 

\noindent \textbf{Binning:} \vspace{-.3cm}
  \begin{enumerate}
  \setlength\itemsep{0cm}
  \item Partition all points with $\delta_1$ quantile bins on feature $X_1$ into index sets\\
  $\{B_1,...B_{\delta_1}\}$ such that $B_\ell = \{i \hspace{.2cm} | \hspace{.2cm} b_{X_1}^q(x_{i1}) = \ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...\delta_1$
  \item Repeat the following for $j = 2,...,p$ :
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
    \item Define $C_{s t} = \{i \hspace{.2cm} | \hspace{.2cm} i \in B_s$ and $b_{X_{j}}^q(x_{ij}) = t \} \hspace{.2cm} \forall \hspace{.2cm} s = 1,...,\displaystyle\prod_{d=1}^{j-1}\delta_d$ and $t = 1,...,\delta_j$ \\
    to subdivide each $B_s$ from previous step with $\delta_j$ quantile bins on feature $X_j$ 
    \item Redefine index sets $\{B_1,...,B_L\}$ such that $B_\ell = C_{s t}$, where $\ell = t(s-1) + t$ \\
    to combine parent and child subscripts of sets into unique subscript
    \end{enumerate}
    \vspace{-.3cm}
\end{enumerate}
  
\noindent \textbf{Outputs:}
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
  \item Bin neighbor sets $\vec{\mathbf{x}}_\ell = \{\vec{x_i} \hspace{.1cm} | \hspace{.1cm}  i \in B_\ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,L$, where $L=\displaystyle\prod_{j=1}^{p}\delta_j$
    \item Hyper-rectangular bins $\ell = 1,...,L$ containing points $x_{ij} \in (\beta_{j\ell1} \hspace{.1cm} , \hspace{.1cm} \beta_{j\ell2}] \hspace{.1cm} \forall \hspace{.1cm}  j=1,...,p$ 
    \end{enumerate}
\vspace{.3cm}  
\caption{$p$-dimensional iterative-quantile binning}
\label{alg:iqalgorithm}
\end{algorithm}

The process generates $p$-dimensional hyper-rectangular bins over the feature space in $\mathbb{R}^p$ through nested univariate partitions which are stored as an R-tree structured list of depth $p$. The interval bounds for feature $X_{j-1}$ are act as a parent node branching down to the $\delta_j$ intervals for feature $X_j$. It is then simple to identify the iterative quantile bin containing a new query point $[q_1,...,q_p]^t = \vec{q} \in \mathbb{R}^p$ by finding the interval at the $j^{th}$ level of the R-tree containing $q_j$ then moving down the branches for each dimension. Figure~\ref{fig:rtree} below to demonstrates the R-tree generated by the iterative-quantile binning algorithm with the simulated data example from above. In this example, the bin containing a query point, $\vec{q}=(q_{1},q_{2})$, can be identified by first finding the interval containing $q_{1}$ in the first level of the R-tree, then finding the interval containing $q_{2}$ among the three nest intervals. 

\begin{figure}[hbtp]
\centering
  \includegraphics[width=.85\textwidth]{./figure/RtreeSketch.pdf}
  \caption{ \ktm{(Rough Sketch of)} Interval R-tree structure generated by iterative-quantile binning in simulated feature data. }
  \label{fig:rtree}
\end{figure}

A benefit of this nested structure is that our search ignores all branches below intervals in the $j^{th}$ level of the R-tree not containing $j^{th}$ dimension of a query point. A query for membership among $\prod_{d=1}^{p}\delta_d$ unique $p$-dimensional bins is completed by checking membership within $\delta_j$ intervals at each level; thus $\sum_{d=1}^{p}\delta_d$ univariate intervals overall. Let $b_{\mathbf{X}}^{iq}(.): \vec{q} \rightarrow \ell$ denote the function mapping query points to iterative quantile bins by passing through the branches of the R-tree of intervals. 

%-----------------------------------------------------------------------
\subsection{Iterative Quantile Nearest-Neighbor Modeling}
\label{iqnnmodels}

In Subsection~\ref{unibin} we used univariate quantile bin neighborhoods to roughly approximate KNN regression and classification models. An extension to $p$-dimensional feature spaces will use summary statistics of the training response values within iterative-quantile bins to predict responses for test points. We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for regression and classification defined such that

\begin{center}
$\hat{y}_{\vec{q}} = S(\vec{\mathbf{y}}_\ell)$, where $\vec{\mathbf{y}}_\ell = \{y_i \hspace{.15cm} | \hspace{.15cm} i = B_{\ell} \text{, where } \ell=b_{\mathbf{X}}^{iq}(q)\}$
\end{center}

Thus estimating the response for a query point, $\hat{y}_{\vec{q}}$, using some appropriate summary statistic $S(.)$ over the response values over training observations from the iterative quantile bin containing the query point. We propose using a simple average of numeric responses for IQNN regression and a majority vote count of categorical responses for IQNN classification. Note that the prediction for any query point in iterative-quantile bin $\ell$ is always $S(\vec{\mathbf{y}}_\ell)$ which may be pre-calculated and stored for fast retrieval using the R-tree structure. The rightmost panel of Figure~\ref{fig:2d_iqbin_plots} visually demonstrates the predicted values from IQNN regression over the simulated bivariate feature space resulting from averaging the ten training data response values in each bin. 

A potential problem for IQNN models is that a test observation could fall beyond the outermost bounds of the hyper-rectangular bins. We could refuse to predict for these outlying points to avoid extrapolation outside the range of observed data; however, this would severely limit the practical applicability of IQNN models. An alternative is to alter the quantile binning function, $b_{X}^q(.)$, used in stages 1 and 2.i of Algorithm~\ref{alg:iqalgorithm} to extend the outermost bin boundaries by some tolerance parameter. A \textit{stretched} quantile binning function, $b_{X,\tau}^q(.):x \rightarrow j$, would be defined as 
%
\begin{eqnarray}
\label{stretchbin}
b_{X,\tau}^q(x) = \left\{\begin{array}{ll} 
  1 & \text{ for all } x_i \in [\min(x_i) - \tau \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{1}{J}) ]  \\
  j & \text{ for all } x_i \in (Q_X(\frac{j-1}{J}) \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{j}{J})] \\
  J & \text{ for all } x_i \in (Q_X(\frac{J-1}{J}) \hspace{.2cm} , \hspace{.2cm} \max(x_i) + \tau] 
\end{array}\right.
\end{eqnarray}
%
Setting the tolerance value, $\tau$, to the outermost quantile bins allows for the practitioner to control how far outside the range of observed points they are willing to make predictions. Additionally, a different value for $\tau$ can be specified for each dimensions to allow for more/less strict tolerance of extrapolation for each feature variable. We could also guarantee that every query point belongs to an iterative-quantile bin by letting $\tau \rightarrow \infty$ for unlimited tolerance.

Iterative-quantile binning provides a potential alternative to $k$-nearest neighborhoods within training data, but it is important to recognize how fundamental differences in how their respective predictive models treat of feature values and how they are parameterized. The magnitude of values in each features dimension heavily influence $p$-dimensional distances between training and query points. Feature values are often standardized before constructing KNN models, in an attempt to allow more balanced contribution across feature dimensions. Interestingly for IQNN models, rescaling will impact the hyper-rectangular bin boundaries but will \textit{not} change the training points in each bin neighborhood. This is because order of training points is invariant to scaling, thus quantile-based partitions result in the same points in each group. We can make a similar attempt to balanced contribution of each features by specifying the IQNN model with equal number of bins in each dimension. We define a \textit{balanced-bin} parameterization with $\delta_1=...=\delta_p=\delta$ for some positive integer and will provide an average of $n/\delta^p$ points per bin. The problem with this parameterization is that the bin counts shift dramatically as $\delta$ changes, giving us much less control over neighborhood sizes than with KNN, where any positive integer, $k$, can be defined. To loosen these constraints we define \textit{nearly-balanced-bin} parameterization such that $\delta_j \in \delta_{j'} \pm \gamma$ of any dimensions $j$ and $j'$. Bin-balance parameter, $\gamma$, is set as some small integer; creating nearly the same number of bins in each dimension, while maintaining more control over bin counts. For example, if we set $\gamma=1$ and $\delta_1=4$ for a bivariate feature space, then $\delta_2=4$ makes balanced-bins, $\delta_2=3$ makes nearly-balanced-bins and $\delta_2=2$ makes unbalanced-bins.
%-----------------------------------------------------------------------------

\section{IQNN Model Properties and Performance}
\label{eval}

An iterative-quantile nearest-neighbor model roughly mimics the neighborhood mechanic of a k-nearest-neighbor model using iterative-quantile bin neighborhoods over the feature space of the training data. To assess the viability of these IQNN models we need to evaluate the computational costs for constructing the binning structure using training observations and the time required and accuracy when making predictions for test observations. We then compare IQNN model performance to KNN, cover-tree AKNN, and kd-tree AKNN using several real data sets. We begin by by investigating the computation times using theoretical and empirical approaches in Subsection~\ref{timing}. We then assess the predictive performance of IQNN models in Subsection~\ref{accuracy}. 


%-----------------------------------------------------------------------
\subsection{Computational Efficiency}
\label{timing}

These models requires computational investment for the construction of the R-tree of univariate intervals over each of the $p$-dimensions used for binning future observations with the iterative-quantile binning function, $b_{\mathbf{X}}^{iq}(q)$. The dominating computational cost for creating the iterative-quantile bins is from generating the $\delta_j$ quantiles within each of the $\prod_{d=1}^{j}\delta_d$ data partitions at each iterative step for $j=1,...,p$. Creating empirical quantile for n observations can be conducted in $O(nlog(n))$ time; a result of using a comparison-based quick-sort algorithm to order the values. Luckily as the number of partitions requiring quantile calculations increases with each iteration, the number of training values per bin are dropping. The combined time required for generating quantiles in all stages of Algorithm~\ref{alg:iqalgorithm} is
%
\begin{center}
$O(nlog(n) + \displaystyle\sum_{j=2}^{p-1} L_j \frac{n}{L_j}\log(\frac{n}{L_j})) \hspace{.25cm} = \hspace{.25cm} O(nlog(n) + n\displaystyle\sum_{j=2}^{p-1}\log(\frac{n}{L_j})) $,
\end{center}
%
where $L_j = \displaystyle\prod_{m=1}^{j}\delta_m$ is the number of partitions at iteration $j$; thus $\frac{n}{L_j}$ is the average number of observations per partition at iteration $j$. Note that because $L_j \ge 2$ when $j \ge 2$, the time is bounded above by $O(pn\log(n))$.    

The storage size of the resulting bin structure will however increase as the number of partitions increase because the $j^{th}$ level of the R-tree requires $O(L_j)$ space; a total space of size $O(\sum_j^p\{L_j)\})$. Note that the computational time required decreases and the computation storage increases as the number of partitions, $\delta_j$, at each iteration increases. Thus partitioning to smaller and more bins requires less time but more storage space.

The IQNN model then performs predictions for test observations in $O(\sum_{j=1}^{p}\delta_j)$ time, scaling with the number of univariate interval checks required to move through the R-tree. Table~\ref{tab:theory_time} summarizes the theoretical pre-processing and prediction times for IQNN models in comparison to the KNN, and AKNN methods discussed in Section~\ref{intro}.

\begin{table}[h]
\small
\centering
\begin{tabular}{rlll} \hline
Model & Pre-processing & Storage & Prediction \\ 
 \hline  
IQNN & $O(nlog(n\prod_{j=2}^{p-1}\frac{n}{L_j}))$ & $O(\sum_j^pL_j)$ & $O(m\sum_{j=1}^{p}\delta_j)$ \\
%-------------
brute force - KNN & - & - & $O(mn(1+log(n)))$ \\
cover-tree - AKNN & $O(c^6 pn\log(n))$ & $O(n)$ & $O(mc^{12} \log(n))$ \\
kd-tree - AKNN    & $O(pn\log(n))$ & $O(n)$ & $O(mc_{p\epsilon}\log(n))$ \\
\hline
\end{tabular}
\caption{Computational costs for pre-processing training data with $n$-rows by $p$-columns, storing searchable structure, and predicting for test data with $m$-rows by $p$-columns.}
\label{tab:theory_time}
\end{table}
\normalsize

We use simulated training and test data sets to empirically evaluate the time required for preprocessing and prediction under many controlled scenarios. For simplicity, values for training and test sets are generated using a uniform distribution over continuous intervals; parameterized such that $x_{ij}$ and $y_i$  are independently and identically distributed $Uniform(0,1)$, for all observations $i=1,...,n$ and features $j=1,2$. We will consider only balanced-bin parameterizations for IQNN where $\delta_1=\delta_2$, to allow the most appropriate comparisons with the KNN methods. We generate a new data set for combinations of training data size and neighborhoods size: $n\in\{2^\eta \hspace{.1cm} | \hspace{.1cm} \eta=4,6,8,10,12,14,16,18,20\}$ and $k\in\{2^\kappa \hspace{.1cm} | \hspace{.1cm} \kappa=2,4,6,8,10,12,14\}$, respectively. This corresponds with between 16 to 1048576 observation in the training data, and between 1 to 16384 neighbors. We maintain a constant number of test points with $m=1000$ because all models being considered generate predictions in line, thus timing differnces will reflect the relative speed per prediction. Note that the powers of two for $n$ and $k$ conveniently align the neighborhood sizes with the $\delta$ parameter needed for balanced binning; for the bivariate feature space $\delta = (n/k)^{1/p} = (2^\eta/2^\kappa)^{1/2} = 2^{(\eta-\kappa)/2}$ will provide matching neighborhood sizes for the IQNN and KNN methods. Naturally, we consider only parameterizations where the training set is larger than the neighborhood sizes, $n > k$. We impose one further constraint to $n$ and $k$ combinations such that we place an upper limit at $2^{14}$ IQNN bins. 

For each parameterization described above we repeatedly simulate training and test data sets, then we collect and average timing measurements for training data pre-processing and generating test data predictions using IQNN, KNN and AKNN models. All simulations were run using R on a Dell Optiplex 9020(TM) Desktop with an Intel(R) Core(TM) i7-4770 3.40GHz CPU \citep{R}. \ktm{IQNN methods were implementated in R and are available in the \texttt{iqbin} package (cite my package here?)}. The KNN and AKNN methods were run using impementations in the \texttt{FNN} package \citep{FNN}, which are highly optimized and drop computationally intensive tasks down into C \citep{kernighan2006c}.  

To account for variation in processing speed we look to the average timing for pre-processing and prediction for each parameterization over repeated simulations and the order of the model fitting was randomized for each iteration to avoid any systemic processing differences. For larger training sets, $n \ge 2^{16}$, we repeat simulations 25 times for each parameterization. For smaller training sets, $n \le 2^{14}$ we repeat all simulations 500 times for each parameterization. We use a larger number of simulations for these small data cases because higher precision is needed to compare average timing values when the average times are all very short and close together, whereas average timing differences manifest much more strongly when $n$ is large. See Table\ktm{AddRefHere} in Appendix\ktm{addAppendixRefHere} for all parameter combinations and average timing measurements.  

Figure~\ref{fig:sim_time} displays the average time for pre-processing and predicting using each method in each simulated scenario. We see that for small training set sizes and for small numbers of neighbors, the KNN and AKNN models outperform the IQNN model. The KNN model using brute-force requires no time to pre-precess but requires substantially more time to predict in the test set as the training set size increases. The IQNN and both AKNN methods have increasing pre-processing time as the training and/or neighborhood sizes grow. Interestingly, the pre-processing time grows more rapidly as a function of $n$ but less rapidly as a function of $k$ for the IQNN model than the AKNN models. Pre-processing in AKNN models tends to be more efficient than IQNN for small training sets and neighborhoods, but less efficient as these increase. For prediction, the AKNN methods require increasing time as training and neighborhood sizes increase, whereas the IQNN models require nearly constant time across all scenarios. As with pre-processing, AKNN models are more efficient for prediction than IQNN for small $n$ and $k$, but less efficient as these sizes grow. 
<<sim_time, echo=F,include=T,eval=T,fig.width=9, fig.height=5, out.width='.99\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Empirical timing measurements from preprocessing and prediction stages using each algorithm on simulated data sets across combinations of neighborhood sizes ($k$) and training set sizes ($n$).",warning=FALSE, message=FALSE>>=

# pull out fittimes and move from wide to tall
sim_fit_times <- sim_times_agg %>%
  select(-ends_with("predtime")) %>%
  gather(key="type",value="time",knn_brute_fittime:iqnn_fittime) %>%
  mutate(stage = "fitting",
         type = str_sub(type,1,-9),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))) )

# pull out prediction times and move from wide to tall
sim_pred_times <- sim_times_agg %>%
  select(-ends_with("fittime")) %>%
  gather(key="type",value="time",knn_brute_predtime:iqnn_predtime) %>%
  mutate(stage = "predicting",
         type = str_sub(type,1,-10),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))),
         dlabel = factor(paste0("delta == 2^",log2(d)), levels=paste0("delta == 2^",sort(unique(log2(sim_times_agg$d))))) )

my_colors <- RColorBrewer::brewer.pal(n=4,name="Set1")
# my_colors <- RColorBrewer::brewer.pal(n=7,name="Set1")[c(1,2,3,7)] # rearrange to highlight iqnn and knn
# The colorblind palette with black: from http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
# my_colors <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# # plot fit times
# fit_times <- ggplot()+
#   geom_hline(yintercept = 0)+
#   # geom_vline(xintercept = 2^14)+
#   geom_line(aes(x=n, y=time, color=type),
#             size=1.2,data=sim_fit_times) +
#   facet_grid(.~klabel, labeller = label_parsed)+
#   scale_y_continuous("Time in log(mins)", trans="log", breaks=c(.0001,.001,.01,.1,1,10)) +
#   scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
#                      labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
#   # scale_color_brewer(palette="Set1")+
#   scale_color_manual(values=my_colors)+
#   labs(title="Training Data Pre-Processing Times", x=" ")+
#   theme_bw()+
#   theme(legend.position = "none")
# 
# # plot pred times
# pred_times <- ggplot()+
#   geom_hline(yintercept = 0)+
#   # geom_vline(xintercept = 2^14)+
#   geom_line(aes(x=n, y=time, color=type),
#             size=1.2,data=sim_pred_times) +
#   # geom_text(aes(x=n, y=time, label=dlabel),
#   #           data=filter(sim_pred_times, type=="iqnn"), parse=TRUE) +
#   facet_grid(.~klabel, labeller = label_parsed)+
#   scale_y_continuous("Time in log(mins)", trans="log", breaks=c(0.0001,.001,.01,.1,1,10,100)) +
#   scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
#                      labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
#   # scale_color_brewer(palette="Set1")+
#   scale_color_manual(values=my_colors)+
#   labs(title="Test Data Prediction Times", x="Training Size (n)")+
#   theme_bw()+
#   theme(legend.position = "bottom")
# grid.arrange(fit_times,pred_times,nrow=2, heights=c(1,1.2))

combined_times <- rbind(data.frame(sim_fit_times,dlabel=NA),
                        sim_pred_times)

combined_times$stage_pretty <- factor(ifelse(combined_times$stage=="fitting","Preprocessing","Prediction"),
                                      levels=c("Preprocessing","Prediction"))
combined_times$type_pretty <- factor(combined_times$type, labels=c("IQNN   ","KNN-brute   ","AKNN-cover   ", "AKNN-kd   "))
 ggplot()+
  geom_hline(yintercept = 0)+
  # geom_vline(xintercept = 2^14)+
  geom_line(aes(x=n, y=time, color=type_pretty),
            size=1.2,data=combined_times) +
  # geom_text(aes(x=n, y=time, label=dlabel),
  #           data=filter(sim_pred_times, type=="iqnn"), parse=TRUE) +
  facet_grid(stage_pretty~klabel, labeller = label_parsed)+
  scale_y_continuous("Time in log(mins)", trans="log", breaks=c(0.0001,.001,.01,.1,1,10,100)) +
  scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
                     labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
  # scale_color_brewer(palette="Set1")+
  scale_color_manual("Neighborhood Algorithm:  ", values=my_colors)+
  labs(x="Training Size (n)")+
  theme_bw()+
  theme(legend.position = "bottom")
@

\ktm{ (Remove only add if asked: Similar timing results hold in higher dimensonal feature spaces, where IQNN outperforms the KNN and AKNN methods for "big $n$ - big $k$" cases. See Appendix YYY for the timing results for analogous explorations using simulations with $p=4$ and $p=8$ dimensional feature spaces.) }


%-----------------------------------------------------------------------
\subsection{Predictive Accuracy}
\label{accuracy}

While computational efficiency is important, a new method must demonstract comparable or favorable predictive accuracy to existing method to be considered viable. In this section we compare test accuracy of the IQNN models against those of KNN and AKNN models; using mean squared error (MSE) and misclassification error rates to assess regression and classification models, respectively. Eight real world data sets with categorical responses and continuous features variables were selected to test classification accuracy; likewise, eight data sets with continuous response values and continuous features variables were selected to test regression accuracy. These data sets range in size from $n=150$ to $n=\ktm{12345}$, and number of features variable from $p=2$ to $p=\ktm{123}$. All data sets come from the University of California Irvin (UCI) Machine Learning Repository \citep{Lichman2013}, and the Knowledge Extraction based on Evolutionary Learning (KEEL) data repository \citep{alcala2011keel}. See Table~\ref{tab:classData} in Appendix~\ref{dataAppend} for a more detailed summary of each data set considered below.

The units associated with each feature varible and the number of feature variables differed across all data sets, so a consistent data pre-processing was implemented to allow for more uniform comparisons of accuracy for IQNN, KNN and AKNN models. First, a center/scale transformation was used to standardize each feature variable to negate the impact of diverse meaurement units on the distance calculations necessary for the KNN and AKNN approaches. Secondly, variable selection was performed to narrow the feature space to only two features per data set to simplify comparison across all eight cases. This was done by fitting a random forest to part of the training data removing all but variables with the two highest importance scores; using the average reductions in MSE and Gini index as importance metrics for regression and classification case, respectively. The added complexity of using high importance variables is done to allow stronger predictions under the constraint than might be had by selecting two features at random. The added benefit of this variable selection approach is that the order of importance scores suggests an feature ordering that will is used in the iterative-quantile binning algorithm.

In model specification we aim for the number of iterative-quantile neighbors and k-nearest neightbors to be as close as possible. Each KNN and AKNN model is set to construct three-nearest neighborhoods, and the IQNN models use the nearly-balanced binning parameterization ($\gamma$=1) that provides three points per bin. Figure~\ref{fig:categorical_accuracy} displays the accuracy results for the three neighbor regression and classification models for all considered real-world data sets. In each plot the accuracy values come from average accuracies across 1000 initializations of ten-fold cross validation for each datas et. These are visualized relative change in accuracy compared to the KNN model - fit with brute force - as the baseline model. For regression, we compare the percent change from the KNN cross-validated MSE to remove differences response units across data sets. For classification, we can simply compare the diffence from KNN in cross-validated percent misclassified. 

<<categorical_accuracy, echo=F,include=T,eval=T,fig.width=9, fig.height=8, out.width='.99\\linewidth', fig.pos='H',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="3-nn examples times...",warning=FALSE>>=
# load(file="../plot_data_3nn.Rdata")
# plot the accuracy/fit time/prediction time
ggplot()+
  geom_hline(yintercept = 0)+
  geom_line(aes(x=data_name, y=value,color=type, group=type),size=1, data=results_all_3nn)+
  facet_grid(metric_pretty ~ ., scales="free_y") +
  theme_bw()+
  labs(title="3-NN Classifier vs IQNN Classifier (~3 per bin)",
       subtitle="Based on averages across 1000 repetitions of 10-fold CV",
       x="Data Set", y="")+
  geom_text(aes(x=data_name, label=n), y=.2,data=label_data)
@

In Figure~\ref{fig:categorical_accuracy} we see that for all regression models \ktm{there are ...}. For the classification models we see that IQNN has mixed performance with \ktm{four} case of lower and \ktm{two} cases with higher accuracy than KNN. Noteably, the misclassification error rates for all IQNN classifier models were consistantly within 0.5\% of the KNN accuracy. The accuracy of IQNN models display very similar behavior to that exhibited by the AKNN models. 


%-----------------------------------------------------------------------------

\section{Discussion and Future Work}
\label{discussion}

We have proposed an alternative to $k$-nearest-neighbors models using iterative-quantile binning. In Section~\ref{eval} we showed that IQNN models provide computational advantage over KNN and AKNN models for situations with large training set sizes and large neighborhoods. We also saw that the predictive accuracy from the IQNN model was ... 

It is important to consider the real-time costs to an analyst in choosing between the IQNN, KNN and AKNN models. The simulation in Subsection~\ref{timing} were reported and compared on a log scale, but the real world wait times for the analyst are experienced linearly. For ``small $n$ - small $k$" scenarios the IQNN was consistently slower, but the differences observed in our simulations were measured in seconds. For ``big $n$ - small $k$" simulations we observed that the kd-tree AKNN model outperformed the IQNN for preprocessing, saving several minutes in some cases. However, IQNN providing minutes to hours of reduced wait time in our simulated ``big $n$ - big $k$" scenarios. Therefore, IQNN provides an intreguing computationally efficient alternative to KNN in situations with large training data where we desire to generate large neighborhoods. 
 
Uniquely, the storage size of the IQNN r-tree and time required for prediction is not tied to the training set size. The aggregation within iterative-quantile bins provides an option for scaling a KNN-style method to cases with truly "big $n$" data scenarios if sufficient computational infrastructure is available at the time of model fitting to create bins. The iterative quantile binning algorithm could be implemented using a database query language to scale to situations with huge numbers of training observations because the iterative-quantile binning algorithm only requires tracking indeces in a sequence of quantile calculation, then calculating simple summary statistics over the resulting index groups. 

In Section~\ref{accuracy} that

\newpage





%----------------------------------------------------------------------------
\newpage
\begin{appendix}
\section{Appendix A: Empirical Data Sets}  
\label{dataAppend}

<<data_tables, eval=F, include=F>>=
library(xtable)
xtable(data.frame(name=all_sets, n=all_sizes))
@

\begin{table}[h]
\small
\centering
\begin{tabular}{lrl} \hline
Data & n & Web Address (Accessed: 10/25/17) \\ 
\hline  
iris & 150 & http://archive.ics.uci.edu/ml/machine-learning-databases/iris/ \\
pima & 768 & http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/ \\
yeast & 1484 & http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/ \\
waveform & 5000 & https://archive.ics.uci.edu/ml/machine-learning-databases/waveform/ \\ 
optdigits & 5620 & http://sci2s.ugr.es/keel/dataset.php?cod=199 \\ 
satimage & 6435 & http://sci2s.ugr.es/keel/dataset.php?cod=71 \\ 
marketing & 6876 & http://sci2s.ugr.es/keel/dataset.php?cod=163 \\ 
youtube & 168286 & https://archive.ics.uci.edu/ml/machine-learning-databases/00335/ \\ 
skin & 245057 & https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation \\ 
\hline
\end{tabular}
\caption{Data sets used in testing classification accuracies.}
\label{tab:classData}
\end{table}
\normalsize

%----------------------------------------------------------------------------

\section{Appendix for Origin Offset Proof}  
%----------------------------------------------------------------------------
%\subsection{}
\label{proof:offset}

Appendix stuff goes here

\end{appendix}

\bibliographystyle{asa}

\spacingset{1} % reference spacing

\bibliography{references}

\end{document}
