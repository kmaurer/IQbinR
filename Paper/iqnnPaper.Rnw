\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,psfrag,epsf,float,wrapfig,subfig,tabularx,ulem}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

 \usepackage[utf8]{inputenc}
 \usepackage{fullpage}
 %\graphicspath{{figure/}}
 %\usepackage{csquotes}
 \usepackage{color}
 \usepackage{hyperref}
 \usepackage{mathrsfs}
 
\newcommand{\blind}{0}
\newcommand{\ktm}[1]{{\color{red} #1}} %red comments: 

<<setup,echo=F,include=F,eval=T>>=
options(scipen=5, digits=5)

# libraries

# source in functions

# Load all data

@

%-----------------------------------------------------------------------------------------------------
%%%% Document
%-----------------------------------------------------------------------------------------------------
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%-----------------------------------------------------------------------------------------------------
%%% Title
%-----------------------------------------------------------------------------------------------------
\if0\blind
{
%opening
\title{Title}
\author{author names}
  \maketitle
} \fi

\if1\blind
{
\title{Binning Strategies and Related Loss for Binned Scatterplots of Large Data}
\author{Author A$^1$, Author B$^{2,3}$, Author C$^2$\\$^1$Affiliation X \\$^2$Affiliation Y \\$^3$Affiliation Z}
  \maketitle
} \fi

%-----------------------------------------------------------------------------------------------------
%%% Abstract
%-----------------------------------------------------------------------------------------------------
\bigskip
\begin{abstract}
Write abstract here 
\end{abstract}

\noindent%
{\it Keywords:} Non-parametric, Predictive Modeling, Partitioning, knn, R-Trees
\vfill

%-----------------------------------------------------------------------------------------------------
% Intro
%-----------------------------------------------------------------------------------------------------
\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{Intro}

The nearest-neighbor search is a classic problem in computer science that has applications within many other fields.\ktm{(example citations)}. Within statistical learning the k-nearest neighbors search is used as the non-parametric method for gathering training observations used to predict for both regression and classifier models. The basic mechanism for $k$-nearest-neighbor (KNN) models is to use numeric feature variables to calculate distances between a new query observation and all the observations in the training data, then create a prediction based on the responses from the k training observations with the smallest distances from the query point \ktm{(add knn citation)}; most typically using an average of response values for KNN regressions, and the majority vote for KNN classifiers \ktm{(general KNN learning citation)}. 

Given the rapid growth in data sizes and availability, it is important to consider how the computational requirements of a statistical learning method scale with increase training and test data sizes. KNN models rely on calculation of $p$-dimensional distances - typically Euclidean - between the $n$ observations in the training set for each new query observation, which can be solved in O(n) time through a brute force approach. This can clearly become computationally demanding when either the training data size increases. The computational demands are compounded when using KNN models to predict for large test data sets. In a test set with $m$ observations, every new query requires recalculation of distances; solved by brute force in O(nm) time. 

Algorithms have been developed to improve the speed at which queries are able to run by pre-processing the training data to develop more efficient tree-based structures for finding the nearest neighbors. Roussopoulos et al. presented the Branch-and-Bound R-tree method, using minimal bounding rectangles to group observations for efficient nearest-neighbor searches. While the method made cleaver use of nested hyper-bins, it was fundamentally centered on finding the single nearest-neighbor and the computational gains erode quickly as query neighborhood sizes increase \ktm{(Roussopoulos, Kelley and Vincent	1995)}. Arya and Mount presented another method for preprocessing the training data into a kd-tree structure for efficient $k$-nearest-neighbor style queries, however their search method finds \textit{approximate k-nearest-neighbors} (AKNN); a set of $k$ observations with distances within a ratio of $(1+\epsilon)$ to the query point as the true $k^{th}$-nearest-neighbor, for some specified $\epsilon$. This kd-tree ANN method requiring $O(n^2)$ time to organize the data into nested conically shaped search regions that are then stored in $O(n\log(n))$ space, and AKNN queries then require only $O(\log^3(n))$ time \ktm{(Arya and Mount	1993)}. Arya et al. then optimized the AKNN algorithm to preprocess the kd-tree in $O(n\log(n))$ time and store in $O(n)$ space, then able to make AKNN queries in $O(c_{p\epsilon}\log(n))$ time, where $c_{p\epsilon}$ is a constant bounded above by a function of the dimensionality, $p$, and the approximation parameter, $\epsilon$ \ktm{(Arya, Mount, Netanyahu, Silverman and Wu	1998)}. Beygelzimer et al. provide the cover-tree method as an alternative for either KNN or AKNN searches. Cover-tree AKNN approach take $O(c^6 n\log(n))$ time to construct the cover-tree, required $O(n)$ storage space and allows nearest-neighbor queries in $O(c^{12} \log(n))$ time, where $c$ is a bounded expansion constant \ktm{(Beygelzimer, Kakade, Langford	2006)}.  

Instance selection methodology provides a fundamentally different option for speeding up KNN queries, aiming to select a targeted subset from the training data on which to compare new observations. By greatly reducing the train set size, $n$, the computational cost of using even a brute force KNN query may be acceptable. Popular instance selection algorithms include \ktm{... Walter suggestions?} 

\ktm{paragraph transitioning to discussions on predictive modeling goals of KNN classification/regression}

We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for classification and regression that provides a rough approximation to $k$-nearest-neighborhood models through the use of an iterative process of partitioning the feature space using empirical-quantiles of the variable from the training data. In section \ktm{(Ref needed)} we explain the intuition and procedural details of the IQNN algorithm. The method requires at most $O(pn\log(n))$ time to construct the nested quantile based partitions on $p$-dimensional features, which are then stored into an R-tree structure using at most $O(pn)$\ktm{???} space, and iterative-quantile neighborhoods queried in $O(\sum_{j=1}^{p}b_j)$ time, where $b_j$ is the number of partitions in the $j^{th}$ feature dimension. In Section \ktm{(Ref needed)} we evaluate the theoretically expected computational costs of the IQNN approach and we then compare the timing performance and predictive accuracy of the IQNN models to the kd-tree AKNN , cover-tree AKNN, and \ktm{some instance selection methods} on a number of real data sets with both numeric and categorical response values. Finally, in Section \ktm{(Ref Needed)} we discuss the results of our computational testing and identify areas for future research.  

\section{Iterative Quantile Nearest-Neighbors}
\label{iqnn}



\subsection{Iterative Quantile Binning}
\label{GenBinning}

Partitioning a continuous range of numeric values using a set of adjacent intervals is commonly refered to as binning. For the univariate case with observations, $x_i$ for $i \in \{1,\dots,n\}$, binning algorithms employ a function $b_X(.) : x_i \rightarrow b_j$ that maps observations to unique bin indeces $j \in \{1,\dots,J\}$. We define a defining a sequence of $J$ adjacent intervals, $(\beta_{j-1},\beta_{j}]$ for bins $j \in \{1,\dots,J\}$. Note that half open intervals are used such that any observation falling on a bin boundary is assigned to a unique interval. Values $x_i$ exactly equal to the lowest bin boundary $\beta_0$ are grouped into the first bin to close the leftmost bound. This is expressed mathematically using the binning function $b_X(.) : x_i \rightarrow x^\ast_j$ defined as 
%
\begin{eqnarray}\label{rectbin}
b_X(x_i) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x_i = \beta_{0} \\
  j & \text{ for all } x_i \in (\beta_{j-1} , \beta_j] 
  \end{array}\right.
\end{eqnarray}  

We define $quantile-binning$ be specifying that the $j^{th}$ bin interval takes the form $(Q_X((j-1)/J),Q_X((j)/J)]$, where $Q_X(p)$ is the the $p^{th}$ empirical quantile using the inverse empirical distribution function \ktm{\citep{hyndman1996sample}}. The resulting partitions may have high variation in interval widths, but will contain an equal or nearly-equal numbers of points, $x_i$. 

For a training data set containing a single continuous numeric feature variable and new query point $q\in\mathbb{R}$, we define the \textit{iterative quantile nearest-neighbors} as the set of training points $\{x_i \hspace{.15cm} | \hspace{.15cm} b_X(x_i) = b_X(q) \}$.

\vspace{1in}

Binning algorithms used in making distributional approximations can be traced back to Pearson's work with the binomial approximation to the normal, where he mentions the need to define an origin and binwidth for segmenting the normal distribution \citep{Pearson1895}. \ktm{Sturges followed with an early work in formalizing histograms specification \citep{sturges1926choice}.}  More recently Scott has presented discussion on the importance of binning specification in the creation of histograms to appropriately display one dimensional density approximations \citep{scott1979}. \citet{scott1992} extends to the properties of multivariate binning strategies.

For the univariate case with observations, $x_i$ for $i \in \{1,\dots,n\}$, binning algorithms require a set of bin centers $x_j^\ast$ for $j \in \{1,\dots,J\}$ and a binning function $b_X(.) : x_i \rightarrow x^\ast_j$ that maps observations to  bin centers. \ktm{What we will refer to as} \textit{general rectangular binning} accomplishes this by defining a sequence of $J$ adjacent intervals, $(\beta_{j-1},\beta_{j}]$ for $j \in \{1,\dots,J\}$, which span over the range of the data. Note that half open intervals are used such that any observation falling on a bin boundary is assigned to a unique interval. Values $x_i$ exactly equal to the lowest bin boundary $\beta_0$ are grouped into the first bin to close the leftmost bound. Each observation is then mapped to a bin center, $x_j^\ast$; the midpoint for the interval to which the observation belongs. 


\textit{Quantile binning} is another option that divides the range of the observations into bins each containing an equal number of points. The $j^{th}$ bin interval takes the form $(Q_X((j-1)/J),Q_X((j)/J)]$, where $Q_X(p)$ is the the $p^{th}$ empirical quantile using the inverse empirical distribution function \ktm{\citep{hyndman1996sample}}. Note that this binning approach is \textit{not} desirable for spatially visualizing density patterns, as it effectively balances the frequency counts in all bins; it does however have desirable properties for binned scatterplots that employ a second stage of binning to create discrete shade scheme for displaying grouped bin frequencies, which will be discussed in Section~\ref{Intro}.

\begin{table}[h]
\centering
\begin{tabular}{lll} \hline
 & Bin Boundaries & Bin Centers \\ 
 \hline  
General &  $ \{\beta_j \text{ }|\text{ } \beta_j > \beta_{j-1} \} $ & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = (\beta_{j-1}+ \beta_j)/2 \}$ \\
%-------------
Standard \hspace{0.5cm} & $ \{\beta_j \text{ }|\text{ } \beta_j = \beta_{j-1} + \omega_X \} $\hspace{0.5cm} & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = \beta_{j-1} + \omega_X/2 \}$ \\
%-------------
Quantile & $ \{\beta_j \text{ }|\text{ } \beta_j = Q_X(j/J) \} $  & $\{x_j^\ast \text{ }|\text{ } x_j^\ast = Q_X((j-0.5)/J) \}$ \\
\hline
\end{tabular}
\caption{Rectangular Binning Specifications}
\label{tab:rectbinning}
\end{table}
%

%-----------------------------------------------------------------------------

\section{Section Title}

% example Figure~\ref{fig:UniformStripes} below.
% \begin{figure}[hbtp]
% \centering
%   \subfloat[Binned scatterplots]{\includegraphics[keepaspectratio=TRUE,width=.63\textwidth]{./figure/CoarseUnif.png}} 
%   \caption[Traditional and adapted scatterplots for games vs. strikeouts data.]{\label{fig:UniformStripes} \ktm{Binned scatterplots and spatial losses for various sized bins with coarse uniform data. Note the slight dip in loss at even integers, when bins are aligned with data resolution.}}
% \end{figure}


<<ExampleCodeChunk, echo=F,include=T,eval=T,fig.width=8, fig.height=4, out.width='.9\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Figure Caption Here.">>= 
hist(1:5)
@


%----------------------------------------------------------------------------
\newpage
\begin{appendix}
\section{Appendix for Origin Offset Proof}  
%----------------------------------------------------------------------------
%\subsection{}
\label{proof:offset}

Appendix stuff goes here

\end{appendix}

\bibliographystyle{asa}

\spacingset{1} % reference spacing

\bibliography{references}

\end{document}
