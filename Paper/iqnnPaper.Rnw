\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx,psfrag,epsf,float,wrapfig,subfig,tabularx,ulem}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{algorithm2e}
\usepackage{array}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

 \usepackage[utf8]{inputenc}
 \usepackage{fullpage}
 %\graphicspath{{figure/}}
 %\usepackage{csquotes}
 \usepackage{color}
 \usepackage{hyperref}
 \usepackage{mathrsfs}
 
\newcommand{\blind}{0}
\newcommand{\ktm}[1]{{\color{red} #1}} %red comments: 

<<setup,echo=F,include=F,eval=T>>=
# options(scipen=5, digits=5)

# libraries
library(iqbin)
library(FNN)
library(tidyverse)
library(stringr)
library(randomForest)
library(RANN)
library(mvtnorm)
library(gridExtra)
library(xtable)

# source in functions

# Load all data
load(file="../sim_all.Rdata")
load(file="../sim_times_agg.Rdata")
@

%-----------------------------------------------------------------------------------------------------
%%%% Document
%-----------------------------------------------------------------------------------------------------
\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%-----------------------------------------------------------------------------------------------------
%%% Title
%-----------------------------------------------------------------------------------------------------
\if0\blind
{
%opening
\title{Iterative-Quantile Nearest-Neighbors}
\author{author names}
  \maketitle
} \fi

\if1\blind
{
\title{Iterative-Quantile Nearest-Neighbors}
\author{Author A$^1$, Author B$^{2,3}$, Author C$^2$\\$^1$Affiliation X \\$^2$Affiliation Y \\$^3$Affiliation Z}
  \maketitle
} \fi

%-----------------------------------------------------------------------------------------------------
%%% Abstract
%-----------------------------------------------------------------------------------------------------
\bigskip
\begin{abstract}
Write abstract here 
\end{abstract}

\noindent%
{\it Keywords:} Non-parametric, Predictive Modeling, Partitioning, knn, R-Trees
\vfill

%-----------------------------------------------------------------------------------------------------
% Intro
%-----------------------------------------------------------------------------------------------------
\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{intro}

The nearest-neighbor search is a classic computational challenge in computer science that has applications within many fields \citep{bhatia2010survey}. Within the context of machine learning the k-nearest neighbors search is used as a non-parametric method for gathering training observations used for prediction in both regression and classifier models (\citealt{fix1951discriminatory},\citealt{cover1967nearest}). The basic mechanism for $k$-nearest-neighbor (KNN) models is to use numeric feature variables to calculate distances between a new query observation and all the observations in the training data, then create a prediction based on the responses from the $k$ training observations with the smallest distances from the query point \citep{kuncheva2004combining}; commonly using an average of response values for KNN regressions, and the majority vote for KNN classifiers (\citealt{friedman2001elements};\citealt{james2013introduction}). 

Given the rapid growth in data volume and availability, it is important to consider how the computational requirements of a statistical learning method scale with increase training and test data sizes. KNN models rely on calculation of $p$-dimensional distances - typically Euclidean - between the $n$ observations in the training set for each new query observation, which can be solved in $O(n)$ time through a brute force approach. Then the distances must be sorted by magnitude to allow the $k$-nearest points be selected, taking an additional $O(nlog(n))$ time with a quick-sort algorithm. This can clearly become computationally demanding when either the training data size increases. The computational demands are compounded when using KNN models to make a large number of predictions. In a test set with $m$ observations, every new query requires recalculation of distances; solved by brute force computation in $O(mn(1+log(n)))$ time. 

Algorithms have been developed to improve the speed at which queries are able to run by pre-processing the training data to allow for more efficient nearest-neighbor searches using tree-based structures. Roussopoulos et al. presented the Branch-and-Bound R-tree method, using minimal bounding rectangles to group observations for efficient nearest-neighbor searches. While the method makes clever use of nested hyper-bins, it is fundamentally focused on finding the single neareroussopoulos1995nearestst-neighbor and the computational gains erode quickly as query neighborhood size, $k$, increases \citep{roussopoulos1995nearest}. Arya and Mount presented another method for pre-processing the training data into a kd-tree structure for efficient \textit{approximate k-nearest-neighbor} (AKNN) queries; a set of $k$ observations with distances within a ratio of $(1+\epsilon)$ to the query point as the true $k^{th}$-nearest-neighbor, for some specified $\epsilon$. This kd-tree AKNN method requires $O(n^2)$ time to organize the data into nested conically shaped search regions that are then stored in $O(n\log(n))$ space, and AKNN queries then require only $O(\log^3(n))$ time \citep{arya1993approximate}. Arya et al. then optimized the AKNN algorithm to pre-process the kd-tree in $O(n\log(n))$ time and store in $O(n)$ space, then able to make AKNN queries in $O(c_{p\epsilon}\log(n))$ time, where $c_{p\epsilon}$ is a constant bounded above by a function of the dimensionality, $p$, and the approximation parameter, $\epsilon$ \citep{arya1998optimal}. Beygelzimer et al. provide the cover-tree method as an alternative for either KNN or AKNN searches. Cover-tree AKNN approach take $O(c^6 n\log(n))$ time to construct the cover-tree, required $O(n)$ storage space and allows nearest-neighbor queries in $O(c^{12} \log(n))$ time, where $c$ is a bounded expansion constant \citep{beygelzimer2006cover}.  
% 
% Instance selection methodology provides a fundamentally different option for speeding up KNN queries, aiming to select a small targeted subsample from the training data on which to compare new observations. By greatly reducing the train set size, $n$, the computational cost of using even a brute force KNN query may be acceptable. Popular instance selection algorithms include \ktm{... Walter suggestions?} 

\ktm{paragraph transitioning to discussions on predictive modeling goals of KNN classification/regression. Include discussion about how KNN is a voroni partitioning of the feature space, can we approximate this partitioning roughly but faster? \citep{aurenhammer1991voronoi}}

We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for classification and regression that provides a rough approximation to $k$-nearest-neighborhood models through the use of an iterative process of partitioning the feature space using empirical-quantiles of the variable from the training data. In Section~\ref{iqnn} we explain the intuition and procedural details of the IQNN algorithm. The method requires at most $O(pn\log(n))$ time to construct the nested quantile based partitions on $p$-dimensional features, which are then stored into an R-tree structure where the iterative-quantile neighborhoods can be queried in $O(\sum_{j=1}^{p}\delta_j)$ time, where $\delta_j$ is the number of partitions in the $j^{th}$ feature dimension. In Section~\ref{eval} we evaluate the theoretically expected computational costs of the IQNN approach and then compare the timing performance and predictive accuracy of the IQNN models to the KNN, kd-tree AKNN, and cover-tree AKNN on a number of simulated and real data sets with both numeric and categorical response values. Finally, in Section~\ref{discussion} we discuss the results of our computational testing and identify areas for future research.  

\section{Iterative Quantile Nearest-Neighbors}
\label{iqnn}

To lay out the proposed method for iterative quantile nearest-neighbor modeling we begin in Subsection~\ref{unibin} with the simple case of a univariate feature space, which requires only a single stage of binned partitioning based on empirical quantiles. Then in Subsection~\ref{iqbin} we explore how the partitioning is iteratively repeated for each subsequent variable in higher dimensional feature spaces. In Subsection~\ref{iqnnmodels} we formalize IQNN models for classification and regression for higher dimensional feature spaces.

\subsection{Univariate Quantile Binned Neighborhoods}
\label{unibin}

Partitioning a continuous range of numeric values using a set of adjacent intervals is commonly referred to as binning. For the univariate case with observations, $x \in \mathbb{R}$ for $i \in \{1,\dots,n\}$, binning algorithms employ a function $b_X(.) : x \rightarrow b_j$ that maps observations to unique bin indices $j \in \{1,\dots,J\}$. We define a defining a sequence of $J$ adjacent intervals, $(\beta_{j-1},\beta_{j}]$ for bins $j \in \{1,\dots,J\}$, and set the outermost bounds, $\beta_{0}=min(x_i)$ and $\beta_{J}=max(x_i)$. Half-open intervals are used such that any observation falling on a bin boundary is assigned to a unique interval and values $x_i$ exactly equal to the lowest bin boundary $\beta_0$ are grouped into the first bin to close the leftmost bound. This is expressed mathematically using the binning function $b_X(.) : x \rightarrow j$ defined as 
%
\begin{eqnarray}\label{rectbin}
b_X(x) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x = \beta_{0} \\
  j & \text{ for all } x \in (\beta_{j-1} , \beta_j] 
  \end{array}\right.
\end{eqnarray}  

It is clear that any new query point, $q \in \mathbb{R}$, can be allocated to a bin by comparing the value to the set of $J+1$ bin bounds. This binning is notably faster than calculating the distances from the training points if $J$ is much smaller than $n$. For cases with one continuous numeric feature with training data points $x_i \in\mathbb{R}$ for $i=1,...,n$ and new query point $q\in\mathbb{R}$, we define the \textit{bin neighbors} as the set of training points $\mathbf{x}_q = \{x_i \hspace{.2cm} | \hspace{.2cm} b_X(x_i) = b_X(q) \}$. It is clear that the distance between a query point and any of its bin neighbors is bounded
\begin{center}
$d(x_i,q) = (x_i - q) < (\beta_j - \beta_{j-1})  \hspace{.2cm} \forall \hspace{.2cm} x_i \in \mathbf{x}_q$,
\end{center}
however the neighbors in each bin may vary depending on bin boundaries and the distribution of training points.Attempting to control $d(x_i,q)$ by setting regularly spaced intervals for any non-uniformly distributed training set would result in imbalanced bin counts; thus, a poor alternative to $k$-nearest neighborhoods. 

We define \textit{quantile binning} through specifying that the $j^{th}$ bin interval, $(\beta_{j-1} , \beta_j]$, takes the form $(Q_X(\frac{j-1}{J})\hspace{.2cm},\hspace{.2cm}Q_X(\frac{j}{J})]$, where $Q_X(p)$ is the the $p^{th}$ empirical quantile using the inverse empirical distribution function \citep{hyndman1996sample}. The quantile binning function $b_X^q(.) : x_i \rightarrow j$ defined as 
%
\begin{eqnarray}\label{quantbin}
b_X^q(x_i) = \left\{\begin{array}{ll} 
  1 &\text{ for all } x_i = \min(x_i) \\
  j & \text{ for all } x_i \in (Q_X(\frac{j-1}{J}) \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{j}{J})] 
  \end{array}\right.
\end{eqnarray}   The resulting $J$ partitions will have interval widths that vary base on distribution, but will each contain either $\lceil \frac{n}{J} \rceil$ or $\lceil \frac{n}{J} \rceil - 1$ training points; thus roughly approximating a $(\frac{n}{J})$-nearest-neighborhood. It should be noted that we are assuming training points $x_i \in \mathbb{R}$ to be truly continuous in nature. If the training data is collected with poor measurement precision or have naturally discrete values, repeated values for $x_i$ could result in empirical quantile where $\beta_j = \beta_k$ for some $j \ne k$, thus non-unique bin assignments. In practice, the simple addition of a small amount of random noise (example: $Uniform(-\eta,\eta)$ , for some small $\eta > 0$) may be added to each value for discrete feature variables to allow for unique quantile bin bounds.

We can then roughly approximate KNN classification or regression modeling by using the quantile bin neighbor response values $\mathbf{y}_q = \{y_i \hspace{.2cm} | \hspace{.2cm} x_i \in \mathbf{x}_q\}$. The query point is given an estimated response, $\hat{y_q} = S(\mathbf{y}_q)$, for some summary statistic $S(.)$ that is appropriate for response type; an average of numeric responses or a majority vote count of categorical responses. The estimated responses for each bin may be pre-calculated and stored for quick retrieval in situations with a large number of query points; unlike KNN models where the estimated response needs to be calculated for each unique query point. 



\subsection{Iterative Quantile Binning}
\label{iqbin}

Now suppose we wish to create binned neighborhoods to approximate $k$-nearest neighborhoods within a $p$-dimensional continuous valued feature space, $\vec{x}_i \in \mathbb{R}^p$. A $p$-dimensional bin can be defined using a $p$-dimensional hyper-rectangle with interval boundaries running orthogonal to the coordinate axes. It would be simplest to generate a univariate binning function for each dimension then apply these independently to form a $p$-dimensional grid of bin boundaries. In many cases the dependence between the feature variables would result in $p$-dimensional bins with highly unbalanced counts, and thus would provide a poor substitute for the equal counts in $k$-nearest neighborhoods. Instead we propose the \textit{iterative-quantile binning} process: starting with the univariate quantile binning described in Subsection~\ref{unibin} above, then iteratively using univariate quantile-bin partitions \textit{nested within} each partition from the previous dimension. By iteratively subdividing nearly equal sized groups at each stage, the resulting $p$-dimensional bins will maintain bin neighborhood counts that are nearly balanced throughout all $p$-dimensions. 

<<2d_iqbin_plots, echo=F,include=T,eval=T,fig.width=3, fig.height=3.5, out.width='.32\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Process for constructing IQNN regression model using 3X3 binning structure. (Left) first partition the training set using tertiles of $X_1$, (Center) then further partition each group using tertiles of $X_2$, (Right) then apply average $Y$ in each bin as the IQNN regression estimates for future points falling within each respective bin.">>=
set.seed(12345)
nsim=90
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "bottom",
                     panel.border = element_blank())

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=.8) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

#iqbin_plot_2d(mybins) rework from here to allow closer match in progression of 3 plots
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=.8)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps, 
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps, 
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+ 
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme
@

Figure~\ref{fig:2d_iqbin_plots} shows this process in the simple case where $\vec{x}_i \in \mathbb{R}^2$ and responses $y_i \in \mathbb{R}$. For this example we have randomly generated \Sexpr{nsim} feature points from a bivariate normal distribution with a strong positive covariance, and linearly related response values with additive random noise; parameterized such that 
\begin{center}
$\vec{x}_i \sim MVN\left(\begin{bmatrix}
  0 \\
  0
\end{bmatrix},\begin{bmatrix}
  1 & .5 \\
  .5 & 1
\end{bmatrix}\right)$, \\
\vspace{.4cm}
$y_i = x_{i1} + x_{i2} + \epsilon_i$, where $\epsilon_i \sim Normal(0,1)$
\end{center}
and iterative quantile binning with three bins per feature dimension is used to create iterative-quantile nearest-neighborhoods of size $k=10$. The left and center panels demonstrates the two stages of quantile-based partitioning necessary for this bivariate feature space. 

This iterative quantile-based process is generalized for binning a set of $p$-dimensional points $[x_{i1},x_{i2},...,x_{ip}]^t = \vec{x}_i \in \mathbb{R}^p \hspace{.2cm} \forall \hspace{.2cm} i \in \{1,...,n\}$ in Algorithm~\ref{alg:iqalgorithm} below.


\RestyleAlgo{boxruled}
\begin{algorithm}
\noindent \textbf{Specification:} Define order of features $\{X_1,X_2,...,X_p\}$ to match desired iterative binning order and number of bins $\{\delta_1,\delta_2,...,\delta_p\}$ for partitioning in each dimension 

\noindent \textbf{Binning:} \vspace{-.3cm}
  \begin{enumerate}
  \setlength\itemsep{0cm}
  \item Partition all points with $\delta_1$ quantile bins on feature $X_1$ into index sets\\
  $\{B_1,...B_{\delta_1}\}$ such that $B_\ell = \{i \hspace{.2cm} | \hspace{.2cm} b_{X_1}^q(x_{i1}) = \ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...\delta_1$
  \item Repeat the following for $j = 2,...,p$ :
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
    \item Define $C_{s t} = \{i \hspace{.2cm} | \hspace{.2cm} i \in B_s$ and $b_{X_{j}}^q(x_{ij}) = t \} \hspace{.2cm} \forall \hspace{.2cm} s = 1,...,\displaystyle\prod_{d=1}^{j-1}\delta_d$ and $t = 1,...,\delta_j$ \\
    to subdivide each $B_s$ from previous step with $\delta_j$ quantile bins on feature $X_j$ 
    \item Redefine index sets $\{B_1,...,B_L\}$ such that $B_\ell = C_{s t}$, where $\ell = t(s-1) + t$ \\
    to combine parent and child subscripts of sets into unique subscript
    \end{enumerate}
    \vspace{-.3cm}
\end{enumerate}
  
\noindent \textbf{Outputs:}
    \vspace{-.3cm}
    \begin{enumerate}[i]
    \setlength\itemsep{0cm}
  \item Bin neighbor sets $\vec{\mathbf{x}}_\ell = \{\vec{x_i} \hspace{.1cm} | \hspace{.1cm}  i \in B_\ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,L$, where $L=\displaystyle\prod_{j=1}^{p}\delta_j$
    \item Hyper-rectangular bins $\ell = 1,...,L$ containing points $x_{ij} \in (\beta_{j\ell1} \hspace{.1cm} , \hspace{.1cm} \beta_{j\ell2}] \hspace{.1cm} \forall \hspace{.1cm}  j=1,...,p$ 
    \end{enumerate}
\vspace{.3cm}  
\caption{$p$-dimensional iterative-quantile binning}
\label{alg:iqalgorithm}
\end{algorithm}

The process generates $p$-dimensional hyper-rectangular bins over the feature space in $\mathbb{R}^p$ through nested univariate partitions which are stored as an R-tree structured list of depth $p$. The interval bounds for feature $X_{j-1}$ are act as a parent node branching down to the $\delta_j$ intervals for feature $X_j$. It is then simple to identify the iterative quantile bin containing a new query point $[q_1,...,q_p]^t = \vec{q} \in \mathbb{R}^p$ by finding the interval at the $j^{th}$ level of the R-tree containing $q_j$ then moving down the branches for each dimension. Figure~\ref{fig:rtree} below to demonstrates the R-tree generated by the iterative-quantile binning algorithm with the simulated data example from above. In this example, the bin containing a query point, $\vec{q}=(q_{1},q_{2})$, can be identified by first finding the interval containing $q_{1}$ in the first level of the R-tree, then finding the interval containing $q_{2}$ among the three nest intervals. 

\begin{figure}[hbtp]
\centering
  \includegraphics[width=.85\textwidth]{./figure/RtreeSketch.pdf}
  \caption{ \ktm{(Rough Sketch of)} Interval R-tree structure generated by iterative-quantile binning in simulated feature data. }
  \label{fig:rtree}
\end{figure}

A benefit of this nested structure is that our search ignores all branches below intervals in the $j^{th}$ level of the R-tree not containing $j^{th}$ dimension of a query point. A query for membership among $\prod_{d=1}^{p}\delta_d$ unique $p$-dimensional bins is completed by checking membership within $\delta_j$ intervals at each level; thus $\sum_{d=1}^{p}\delta_d$ univariate intervals overall. Let $b_{\mathbf{X}}^{iq}(.): \vec{q} \rightarrow \ell$ denote the function mapping query points to iterative quantile bins by passing through the branches of the R-tree of intervals. 

%-----------------------------------------------------------------------
\subsection{Iterative Quantile Nearest-Neighbor Modeling}
\label{iqnnmodels}

In Subsection~\ref{unibin} we used univariate quantile bin neighborhoods to roughly approximate KNN regression and classification models. An extension to $p$-dimensional feature spaces will use summary statistics of the training response values within iterative-quantile bins to predict responses for test points. We propose \textit{iterative-quantile nearest-neighbor} (IQNN) models for regression and classification defined such that

\begin{center}
$\hat{y}_{\vec{q}} = S(\vec{\mathbf{y}}_\ell)$, where $\vec{\mathbf{y}}_\ell = \{y_i \hspace{.15cm} | \hspace{.15cm} i = B_{\ell} \text{, where } \ell=b_{\mathbf{X}}^{iq}(q)\}$
\end{center}

Thus estimating the response for a query point, $\hat{y}_{\vec{q}}$, using some appropriate summary statistic $S(.)$ over the response values over training observations from the iterative quantile bin containing the query point. We propose using a simple average of numeric responses for IQNN regression and a majority vote count of categorical responses for IQNN classification. Note that the prediction for any query point in iterative-quantile bin $\ell$ is always $S(\vec{\mathbf{y}}_\ell)$ which may be pre-calculated and stored for fast retrieval using the R-tree structure. The rightmost panel of Figure~\ref{fig:2d_iqbin_plots} visually demonstrates the predicted values from IQNN regression over the simulated bivariate feature space resulting from averaging the ten training data response values in each bin. 

A potential problem for IQNN models is that a test observation could fall beyond the outermost bounds of the hyper-rectangular bins. We could refuse to predict for these outlying points to avoid extrapolation outside the range of observed data; however, this would severely limit the practical applicability of IQNN models. An alternative is to alter the quantile binning function, $b_{X}^q(.)$, used in stages 1 and 2.i of Algorithm~\ref{alg:iqalgorithm} to extend the outermost bin boundaries by some tolerance parameter. A \textit{stretched} quantile binning function, $b_{X,\tau}^q(.):x \rightarrow j$, would be defined as 
%
\begin{eqnarray}
\label{stretchbin}
b_{X,\tau}^q(x) = \left\{\begin{array}{ll} 
  1 & \text{ for all } x_i \in [\min(x_i) - \tau \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{1}{J}) ]  \\
  j & \text{ for all } x_i \in (Q_X(\frac{j-1}{J}) \hspace{.2cm} , \hspace{.2cm} Q_X(\frac{j}{J})] \\
  J & \text{ for all } x_i \in (Q_X(\frac{J-1}{J}) \hspace{.2cm} , \hspace{.2cm} \max(x_i) + \tau] 
\end{array}\right.
\end{eqnarray}
%
Setting the tolerance value, $\tau$, to the outermost quantile bins allows for the practitioner to control how far outside the range of observed points they are willing to make predictions. Additionally, a different value for $\tau$ can be specified for each dimensions to allow for more/less strict tolerance of extrapolation for each feature variable. We could also guarantee that every query point belongs to an iterative-quantile bin by letting $\tau \rightarrow \infty$ for unlimited tolerance.

Iterative-quantile binning provides a potential alternative to $k$-nearest neighborhoods within training data, but it is important to recognize how fundamental differences in how their respective predictive models treat of feature values and how they are parameterized. The magnitude of values in each features dimension heavily influence $p$-dimensional distances between training and query points. Feature values are often standardized before constructing KNN models, in an attempt to allow more balanced contribution across feature dimensions. Interestingly for IQNN models, rescaling will impact the hyper-rectangular bin boundaries but will \textit{not} change the training points in each bin neighborhood. This is because order of training points is invariant to scaling, thus quantile-based partitions result in the same points in each group. We can make a similar attempt to balanced contribution of each features by specifying the IQNN model with equal number of bins in each dimension. We define a \textit{balanced-bin} parameterization with $\delta_1=...=\delta_p=\delta$ for some positive integer and will provide an average of $n/\delta^p$ points per bin. The problem with this parameterization is that the bin counts shift dramatically as $\delta$ changes, giving us much less control over neighborhood sizes than with KNN, where any positive integer, $k$, can be defined. To loosen these constraints we define \textit{nearly-balanced-bin} parameterization such that $\delta_j \in \delta_{j'} \pm \gamma$ of any dimensions $j$ and $j'$. Bin-balance parameter, $\gamma$, is set as some small integer; creating nearly the same number of bins in each dimension, while maintaining more control over bin counts. For example, if we set $\gamma=1$ and $\delta_1=4$ for a bivariate feature space, then $\delta_2=4$ makes balanced-bins, $\delta_2=3$ makes nearly-balanced-bins and $\delta_2=2$ makes unbalanced-bins.
%-----------------------------------------------------------------------------

\section{IQNN Model Properties and Performance}
\label{eval}

An iterative-quantile nearest-neighbor model roughly mimics the neighborhood mechanic of a k-nearest-neighbor model using iterative-quantile bin neighborhoods over the feature space of the training data. To assess the viability of these IQNN models we need to evaluate the computational costs for constructing the binning structure using training observations and the time required and accuracy when making predictions for test observations. We then compare IQNN model performance to KNN, cover-tree AKNN, and kd-tree AKNN using several real data sets. We begin by by investigating the computation times using theoretical and empirical approaches in Subsection~\ref{timing}. We then assess the predictive performance of IQNN models in Subsection~\ref{accuracy}. 


%-----------------------------------------------------------------------
\subsection{Computational Efficiency}
\label{timing}

These models requires computational investment for the construction of the R-tree of univariate intervals over each of the $p$-dimensions used for binning future observations with the iterative-quantile binning function, $b_{\mathbf{X}}^{iq}(q)$. The dominating computational cost for creating the iterative-quantile bins is from generating the $\delta_j$ quantiles within each of the $\prod_{d=1}^{j}\delta_d$ data partitions at each iterative step for $j=1,...,p$. Creating empirical quantile for n observations can be conducted in $O(nlog(n))$ time; a result of using a comparison-based quick-sort algorithm to order the values. Luckily as the number of partitions requiring quantile calculations increases with each iteration, the number of training values per bin are dropping. The combined time required for generating quantiles in all stages of Algorithm~\ref{alg:iqalgorithm} is
%
\begin{center}
$O(nlog(n) + \displaystyle\sum_{j=2}^{p-1} L_j \frac{n}{L_j}\log(\frac{n}{L_j})) \hspace{.25cm} = \hspace{.25cm} O(nlog(n) + n\displaystyle\sum_{j=2}^{p-1}\log(\frac{n}{L_j})) $,
\end{center}
%
where $L_j = \displaystyle\prod_{d=1}^{j}\delta_d$ is the number of partitions at iteration $j$; thus $\frac{n}{L_j}$ is the average number of observations per partition at iteration $j$. Note that because $L_j \ge 2$ when $j \ge 2$, the time is bounded above by $O(pn\log(n))$.    

The storage size of the resulting bin structure will however increase as the number of partitions increase because the $j^{th}$ level of the R-tree requires $O(L_j)$ space; a total space of size $O(\sum_j^p\{L_j)\})$. Note that the computational time required decreases and the computation storage increases as the number of partitions, $\delta_j$, at each iteration increases. Thus partitioning to smaller and more bins requires less time but more storage space.

The IQNN model then performs predictions for test observations in $O(\sum_{j=1}^{p}\delta_j)$ time, scaling with the number of univariate interval checks required to move through the R-tree. Table~\ref{tab:theory_time} summarizes the theoretical pre-processing and prediction times for IQNN models in comparison to the KNN, and AKNN methods discussed in Section~\ref{intro}.

\begin{table}[h]
\small
\centering
\begin{tabular}{rlll} \hline
Model & Pre-processing & Storage & Prediction \\ 
 \hline  
IQNN & $O(nlog(n\prod_{j=2}^{p-1}\frac{n}{L_j}))$ & $O(\sum_j^pL_j)$ & $O(m\sum_{j=1}^{p}\delta_j)$ \\
%-------------
brute force - KNN & - & - & $O(mn(1+log(n)))$ \\
cover-tree - AKNN & $O(c^6 pn\log(n))$ & $O(n)$ & $O(mc^{12} \log(n))$ \\
kd-tree - AKNN    & $O(pn\log(n))$ & $O(n)$ & $O(mc_{p\epsilon}\log(n))$ \\
\hline
\end{tabular}
\caption{Computational costs for pre-processing training data with $n$-rows by $p$-columns, storing searchable structure, and predicting for test data with $m$-rows by $p$-columns.}
\label{tab:theory_time}
\end{table}
\normalsize

We use simulated training and test data sets to empirically evaluate the time required for preprocessing and prediction under many controlled scenarios. For simplicity, values for training and test sets are generated using a uniform distribution over continuous intervals; parameterized such that $x_{ij}$ and $y_i$  are independently and identically distributed $Uniform(0,1)$, for all observations $i=1,...,n$ and features $j=1,2$. We will consider only balanced-bin parameterizations for IQNN where $\delta_1=\delta_2$, to allow the most appropriate comparisons with the KNN methods. We generate a new data set for combinations of training data size and neighborhoods size: $n\in\{2^\eta \hspace{.1cm} | \hspace{.1cm} \eta=4,6,8,10,12,14,16,18,20\}$ and $k\in\{2^\kappa \hspace{.1cm} | \hspace{.1cm} \kappa=2,4,6,8,10,12,14\}$, respectively. This corresponds with between 16 to 1048576 observation in the training data, and between 1 to 16384 neighbors. We maintain a constant number of test points with $m=1000$ because all models being considered generate predictions in line, thus timing differnces will reflect the relative speed per prediction. Note that the powers of two for $n$ and $k$ conveniently align the neighborhood sizes with the $\delta$ parameter needed for balanced binning; for the bivariate feature space $\delta = (n/k)^{1/p} = (2^\eta/2^\kappa)^{1/2} = 2^{(\eta-\kappa)/2}$ will provide matching neighborhood sizes for the IQNN and KNN methods. Naturally, we consider only parameterizations where the training set is larger than the neighborhood sizes, $n > k$. We impose one further constraint to $n$ and $k$ combinations such that we place an upper limit at $2^{14}$ IQNN bins. 

For each parameterization described above we repeatedly simulate training and test data sets, then we collect and average timing measurements for training data pre-processing and generating test data predictions using IQNN, KNN and AKNN models. All simulations were run using R on a Dell Optiplex 9020(TM) Desktop with an Intel(R) Core(TM) i7-4770 3.40GHz CPU \citep{R}. \ktm{IQNN methods were implementated in R and are available in the \texttt{iqbin} package (cite my package here?)}. The KNN and AKNN methods were run using impementations in the \texttt{FNN} package \citep{FNN}, which are highly optimized and drop computationally intensive tasks down into C \citep{kernighan2006c}.  

To account for variation in processing speed we look to the average timing for pre-processing and prediction for each parameterization over repeated simulations and the order of the model fitting was randomized for each iteration to avoid any systemic processing differences. For larger training sets, $n \ge 2^{16}$, we repeat simulations 25 times for each parameterization. For smaller training sets, $n \le 2^{14}$ we repeat all simulations 500 times for each parameterization. We use a larger number of simulations for these small data cases because higher precision is needed to compare average timing values when the average times are all very short and close together, whereas average timing differences manifest much more strongly when $n$ is large. See Table~\ref{tab:sim_times_table} in Appendix~\ref{timingAppend} for all parameter combinations and average timing measurements.  

Figure~\ref{fig:sim_time} displays the average time for pre-processing and predicting using each method in each simulated scenario. We see that for small training set sizes and for small numbers of neighbors, the KNN and AKNN models outperform the IQNN model. The KNN model using brute-force requires no time to pre-precess but requires substantially more time to predict in the test set as the training set size increases. The IQNN and both AKNN methods have increasing pre-processing time as the training and/or neighborhood sizes grow. Interestingly, the pre-processing time grows more rapidly as a function of $n$ but less rapidly as a function of $k$ for the IQNN model than the AKNN models. Pre-processing in AKNN models tends to be more efficient than IQNN for small training sets and neighborhoods, but less efficient as these increase. For prediction, the AKNN methods require increasing time as training and neighborhood sizes increase, whereas the IQNN models require nearly constant time across all scenarios. As with pre-processing, AKNN models are more efficient for prediction than IQNN for small $n$ and $k$, but less efficient as these sizes grow. 
<<sim_time, echo=F,include=T,eval=T,fig.width=9, fig.height=5, out.width='.99\\linewidth', fig.pos='h',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Empirical timing measurements from pre-processing and prediction stages using each algorithm on simulated data sets across combinations of neighborhood sizes ($k$) and training set sizes ($n$).",warning=FALSE, message=FALSE>>=

# pull out fittimes and move from wide to tall
sim_fit_times <- sim_times_agg %>%
  select(-ends_with("predtime")) %>%
  gather(key="type",value="time",knn_brute_fittime:iqnn_fittime) %>%
  mutate(stage = "fitting",
         type = str_sub(type,1,-9),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))) )

# pull out prediction times and move from wide to tall
sim_pred_times <- sim_times_agg %>%
  select(-ends_with("fittime")) %>%
  gather(key="type",value="time",knn_brute_predtime:iqnn_predtime) %>%
  mutate(stage = "predicting",
         type = str_sub(type,1,-10),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))),
         dlabel = factor(paste0("delta == 2^",log2(d)), levels=paste0("delta == 2^",sort(unique(log2(sim_times_agg$d))))) )

my_colors <- RColorBrewer::brewer.pal(n=4,name="Set1")[c(1,4,2,3)]
# my_colors <- RColorBrewer::brewer.pal(n=7,name="Set1")[c(1,2,3,7)] # rearrange to highlight iqnn and knn
# The colorblind palette with black: from http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/
# my_colors <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# # plot fit times
# fit_times <- ggplot()+
#   geom_hline(yintercept = 0)+
#   # geom_vline(xintercept = 2^14)+
#   geom_line(aes(x=n, y=time, color=type),
#             size=1.2,data=sim_fit_times) +
#   facet_grid(.~klabel, labeller = label_parsed)+
#   scale_y_continuous("Time in log(mins)", trans="log", breaks=c(.0001,.001,.01,.1,1,10)) +
#   scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
#                      labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
#   # scale_color_brewer(palette="Set1")+
#   scale_color_manual(values=my_colors)+
#   labs(title="Training Data Pre-Processing Times", x=" ")+
#   theme_bw()+
#   theme(legend.position = "none")
# 
# # plot pred times
# pred_times <- ggplot()+
#   geom_hline(yintercept = 0)+
#   # geom_vline(xintercept = 2^14)+
#   geom_line(aes(x=n, y=time, color=type),
#             size=1.2,data=sim_pred_times) +
#   # geom_text(aes(x=n, y=time, label=dlabel),
#   #           data=filter(sim_pred_times, type=="iqnn"), parse=TRUE) +
#   facet_grid(.~klabel, labeller = label_parsed)+
#   scale_y_continuous("Time in log(mins)", trans="log", breaks=c(0.0001,.001,.01,.1,1,10,100)) +
#   scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
#                      labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
#   # scale_color_brewer(palette="Set1")+
#   scale_color_manual(values=my_colors)+
#   labs(title="Test Data Prediction Times", x="Training Size (n)")+
#   theme_bw()+
#   theme(legend.position = "bottom")
# grid.arrange(fit_times,pred_times,nrow=2, heights=c(1,1.2))

combined_times <- rbind(data.frame(sim_fit_times,dlabel=NA),
                        sim_pred_times)

combined_times$stage_pretty <- factor(ifelse(combined_times$stage=="fitting","Preprocessing","Prediction"),
                                      levels=c("Preprocessing","Prediction"))
combined_times$type_pretty <- factor(combined_times$type, labels=c("IQNN   ","KNN-brute   ","AKNN-cover   ", "AKNN-kd   "))
 ggplot()+
  geom_hline(yintercept = 0)+
  # geom_vline(xintercept = 2^14)+
  geom_line(aes(x=n, y=time, color=type_pretty),
            size=1.2,data=combined_times) +
  # geom_text(aes(x=n, y=time, label=dlabel),
  #           data=filter(sim_pred_times, type=="iqnn"), parse=TRUE) +
  facet_grid(stage_pretty~klabel, labeller = label_parsed)+
  scale_y_continuous("Time in log(mins)", trans="log", breaks=c(0.0001,.001,.01,.1,1,10,100)) +
  scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
                     labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
  # scale_color_brewer(palette="Set1")+
  scale_color_manual("Neighborhood Algorithm:  ", values=my_colors)+
  labs(x="Training Size (n)")+
  theme_bw()+
  theme(legend.position = "bottom")
@

% \ktm{ (Remove only add if asked: Similar timing results hold in higher dimensonal feature spaces, where IQNN outperforms the KNN and AKNN methods for "big $n$ - big $k$" cases. See Appendix YYY for the timing results for analogous explorations using simulations with $p=4$ and $p=8$ dimensional feature spaces.) }
%-----------------------------------------------------------------------
\subsection{Empirical Assessment of Predictive Accuracy}
\label{accuracy}

While computational efficiency is important, a new nearest-neighbor method must demonstract comparable or favorable predictive accuracy to existing methods to be considered viable. In this section we compare test accuracy of the IQNN models against those of KNN and AKNN models using standard loss metrics; mean squared error (MSE) to assess regression models and misclassification error rates to assess classifiers. Ten real data sets with categorical responses and continuous features variables were selected to test classification accuracy; likewise, ten data sets with continuous response values and continuous features variables were selected to test regression accuracy. All data sets come from the University of California Irvin (\textit{UCI}) Machine Learning Repository and the Knowledge Extraction based on Evolutionary Learning (\textit{KEEL}) data repository (\citealt{Lichman2013}; \citealt{alcala2011keel}).  See Table~\ref{tab:data_repos} in Appendix~\ref{dataAppend} for a summary and source citation for each data set used for testing classification and regression. 

To provide consistent comparison, we select and rescale two continuous numeric feature variables from each data set to be used for making predictions in each data scenario. First, we consider only feature variables that are numeric with at least $\sqrt{n}/10$ unique values, thus displaying relatively continuous behavior. Then, we standardize each feature variable to negate the impact of diverse meaurement units on the distance calculations necessary for the KNN and AKNN approaches. Next, variable selection was performed to narrow the feature space to only two features per data set. This was done by selecting the variables with the two highest importance scores from a random forest fit to the training data; using the average reductions in MSE and Gini index as importance metrics for regression and classification case, respectively. While the variable selection method is somewhat arbitrary here, using high importance variables should provide models with strong predictions under the bivariate feature constraint. 
%Additionally, we elect to specify the iterative-quantile algorithm to bin the features in order of importance scores. 
See Table~\ref{tab:tuning_reg_class} in Appendix~\ref{accuracyAppend} contains the selected pair of feature variables that are used when testing the predictive accuracy of each model. 

\subsubsection{Classification Accuracy}
\label{classacc}

To prepare for comparing the nearest-neighbor classification models, we first tuned the parameterization for each classifier using ten initializations of ten-fold cross-validation and make the final model selection using the parameterization with the minimum average misclassification rate. For IQNN this entailed tuning across ($\delta_1$,$\delta_2$) pairs that control the number of bins in each feature dimesion. We narrowed our tuning to select from only nearly-balanced bins ($\gamma=1$) that provide average binned-neighborhood sizes that fell nearest to whole integers. This was done to avoid tuning exhaustively over all possible bin parameterizations. We then tuned the KNN models over the set of neighborhood sizes, $k$, that corresponded to the integers nearest to the average binned-neighborhood sizes from the IQNN tuning. In this way we tune over the same number of similar-sized neighborhoods for each method. The misclassification rates from the tuned models were then collected for 100 additional initializations of ten-fold cross-validation for each data set. See Table~\ref{tab:tuning_reg_class} in Appendix~\ref{accuracyAppend} for the parameters selected in tuning and average ten-fold cross-validated misclassification error rates from each model on each data set.

Figure~\ref{fig:categorical_accuracy} displays the accuracy results for the tuned classification models for all considered data sets. This visualization emphasizes the difference in average cross-validated percent misclassified relative to brute-force KNN as a baseline. We see that the data sets are shown from left to right in descending order of misclassification rates; indicating a relative level of difficulty for each classification task. From this we see that the IQNN method performs very similar to the KNN and AKNN methods; between 0.3\% better and 1.8\% worse for all sets tested. In three of the ten scenarios tuned IQNN model has a lower error rate than all other tuned KNN and AKNN models.  

 <<categorical_accuracy, echo=F,include=T,eval=T,fig.width=9, fig.height=5.5, out.width='.99\\linewidth', fig.pos='H',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Average misclassification error rates from 100 initializations of ten-fold cross-validatation on all tuned models (top) and difference in misclassification error rates relative to KNN as a baseline (bottom).",warning=FALSE>>=
load(file="../DataRepo/classification/tuned_classification_testing.Rdata") 
 
#Setting order of data sets
iqnn_class_err <- filter(results_class, type=="iqnn") %>% arrange(avg_cv_accuracy)
iqnn_err_order <- as.character(iqnn_class_err$data_name)
 
# clean data for plots of accuracy relative to knn as baseline
#   - separate knn from others for "baseline" layers vs "points" layers in plotting
class_plot_data_knn <- results_class %>%
  filter(type == "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_err_order),
         x=as.numeric(data_name)-.5,
         xend=as.numeric(data_name)+.5)
class_plot_data_knn$type_pretty <- "KNN-brute"

shiftval=.25
class_plot_data <- results_class %>%
  filter(type != "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_err_order),
         shift = (as.numeric(as.factor(as.character(type)))-2)*shiftval,
         diff_err_perc = -diff_acc*100)
class_plot_data$type_pretty <- factor(class_plot_data$type, labels=c("IQNN   ","AKNN-cover ", "AKNN-kd   ")) 

x_label_sizes <- as.numeric(sapply(levels(class_plot_data$data_name), function(x) class_plot_data$obs[class_plot_data$data_name==x][1]))
my_x_ticks <- paste0(levels(class_plot_data$data_name), "\n n=",x_label_sizes)

# plot relative to KNN
#   - use vertical lines to partition between datasets
#   - baseline KNN-brute as flat line segment
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p1 <- ggplot()+
  geom_segment(aes(x=x, xend=xend,y=100-avg_cv_accuracy*100,yend=100-avg_cv_accuracy*100, linetype="KNN-brute"),
               color=RColorBrewer::brewer.pal(4,"Set1")[4], size=1, data=class_plot_data_knn) +
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=100-avg_cv_accuracy*100,color=type_pretty,shape=type_pretty),
             size=3,data=class_plot_data)+
  theme_bw() +
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous("", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5)) +
  scale_y_continuous("Misclassification Rates\n (units of % error)",
                     breaks=seq(0,100,by=10),labels=paste0("  ",seq(0,100,by=10),"% ")) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(17,15,19))+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "none")


# plot relative to KNN
#   - use vertical lines to partition between datasets
#   - baseline at 0 to represent KNN-brute
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p2 <- ggplot()+
  geom_segment(aes(x=.5, xend=10.5,y=0,yend=0, linetype="KNN-brute"),size=1, color=RColorBrewer::brewer.pal(4,"Set1")[4])+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=diff_err_perc,color=type_pretty,shape=type_pretty),
             size=3,data=class_plot_data)+
  theme_bw()+
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous(" ", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5))+
  scale_y_continuous("Difference from KNN-brute\n (units of % error)",
                     breaks=seq(-0.5,1.5,by=.5),labels=c("-0.5%","0.0%","0.5%","1.0%","1.5%"),
                     limits=c(min(-.5,min(class_plot_data$diff_err_perc)),max(1.5,max(class_plot_data$diff_err_perc)))) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(17,15,19))+
  # annotate(geom="text",x=0,y=0,label="bold(KNN-brute)", color=RColorBrewer::brewer.pal(4,"Set1")[4], parse=T,vjust=-.2,hjust=0.4)+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "bottom")

grid.arrange(p1,p2,nrow=2,heights=c(1,1.2))

@


%-----------------------------------------------------------------------------

\subsubsection{Regression Accuracy}
\label{regacc}

To compare the predictive accuracy across the nearest-neighbor style regresson models we collect the root mean squared error (RMSE) for the ten regression data sets in an analogous process to that used for classification in subsection~\ref{classacc} above. We first clean, rescale and select the feature variables for each regression data set. We additionally standardized the continous response value to eliminate the inconsistent units when comparing across data sets; thus the RMSE is in units of standard-deviation for all sets. The tuning process for each model again selects the parameterizations that minimize the average RMSE from ten initializations of ten-fold cross-validation. Then an additional 100 initializations of ten-fold cross-validation are run for each tuned model to assess average performance of IQNN, KNN and AKNN methods on each data set. See Table~\ref{tab:tuning_reg_class} in Appendix~\ref{accuracyAppend} the parameters selected in tuning and average ten-fold cross-validated RMSE values from each regression model on each data set.

In Figure~\ref{fig:regression_accuracy} we see that the IQNN made less accurate predictions than the KNN for most of the regression data sets; however the loss of accuracy was small relative to the units of the response variable. Typically the RMSE for IQNN is within 0.02 response standard deviations from the KNN and AKNN models. The Wisconsin Prognostic Breast Cancer (wpbc) data is the case with the difference in predictive accuracy with the tuned IQNN model have an RMSE value of 0.039 standardized units higher than the tuned KNN model.

 <<regression_accuracy, echo=F,include=T,eval=T,fig.width=9, fig.height=5.5, out.width='.99\\linewidth', fig.pos='H',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap="Average RMSE from 100 initializations of ten-fold cross-validatation on all tuned models (top) and difference in RMSE relative to KNN as a baseline (bottom).",warning=FALSE>>=
load(file="../DataRepo/regression/tuned_regression_testing.Rdata") 
 
#Setting order of data sets
iqnn_reg_rmse <- filter(results_reg, type=="iqnn") %>% arrange(desc(rmse))
iqnn_rmse_order <- as.character(iqnn_reg_rmse$data_name)

# clean data for plots of RMSE relative to knn as baseline
#   - separate knn from others for "baseline" layers vs "points" layers in plotting
reg_plot_data_knn <- results_reg %>%
  filter(type == "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_rmse_order),
         x=as.numeric(data_name)-.5,
         xend=as.numeric(data_name)+.5)
reg_plot_data_knn$type_pretty <- "KNN-brute"

shiftval=.25
reg_plot_data <- results_reg %>%
  filter(type != "knn - brute") %>%
  mutate(shift = (as.numeric(as.factor(as.character(type)))-2)*shiftval,
         data_name = factor(data_name,levels=iqnn_rmse_order))
reg_plot_data$type_pretty <- factor(reg_plot_data$type, labels=c("IQNN   ","AKNN-cover ", "AKNN-kd   ")) 

x_label_sizes_reg <- as.numeric(sapply(levels(reg_plot_data$data_name), function(x) reg_plot_data$obs[reg_plot_data$data_name==x][1]))
my_x_ticks_reg <- paste0(levels(reg_plot_data$data_name), "\n n=",x_label_sizes_reg)


# plot relative to KNN on raw RMSE scale
#   - use vertical lines to partition between datasets
#   - baseline KNN-brute as flat line segment
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p3 <- ggplot()+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_segment(aes(x=x, xend=xend,y=avg_cv_mse,yend=avg_cv_mse, linetype="KNN-brute"),
               color=RColorBrewer::brewer.pal(4,"Set1")[4], size=1, data=reg_plot_data_knn) +
  geom_point(aes(x=as.numeric(data_name)+shift,y=avg_cv_mse,color=type_pretty,shape=type_pretty),
             size=3,data=reg_plot_data)+
  theme_bw() +
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous("", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5)) +
  labs(y="Average RMSE\n (units of std dev)")+
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(17,15,19))+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "none")


# plot relative to KNN as difference in RMSE from KNN-brute
#   - use vertical lines to partition between datasets
#   - baseline at 0 to represent KNN-brute
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p4 <- ggplot()+
  geom_segment(aes(x=.5, xend=10.5,y=0,yend=0, linetype="KNN-brute"),size=1, color=RColorBrewer::brewer.pal(4,"Set1")[4])+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=rmse_diff,color=type_pretty,shape=type_pretty),
             size=3,data=reg_plot_data)+
  theme_bw() +
  scale_x_continuous(" ", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5)) +
  labs(y="Difference from KNN-brute\n (units of std dev)")+
  # scale_y_continuous("Relative CV-RMSE \n (Difference from KNN-brute)", breaks=seq(-.01,.04,by=.01),limits=c(-0.01,0.04)) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(17,15,19))+
  scale_linetype_manual("Baseline: ", values=1)+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "bottom") +
  annotate(geom="text",x=0,y=0,label="bold(KNN-brute)", color=RColorBrewer::brewer.pal(4,"Set1")[4], parse=T,vjust=-.2,hjust=0.4)

grid.arrange(p3,p4,nrow=2,heights=c(1,1.2))

@

%-----------------------------------------------------------------------------

\section{Discussion and Future Work}
\label{discussion}

We have proposed an alternative to $k$-nearest-neighbors models using iterative-quantile binning. In Section~\ref{eval} we showed that IQNN models provide computational advantage over KNN and AKNN models for situations with large training set sizes and large neighborhoods. We also saw that the predictive accuracy from the IQNN model was ... 

It is important to consider the real-time costs to an analyst in choosing between the IQNN, KNN and AKNN models. The simulation in Subsection~\ref{timing} were reported and compared on a log scale, but the real world wait times for the analyst are experienced linearly. For ``small $n$ - small $k$" scenarios the IQNN was consistently slower, but the differences observed in our simulations were measured in seconds. For ``big $n$ - small $k$" simulations we observed that the kd-tree AKNN model outperformed the IQNN for preprocessing, saving several minutes in some cases. However, IQNN providing minutes to hours of reduced wait time in our simulated ``big $n$ - big $k$" scenarios. Therefore, IQNN provides an intreguing computationally efficient alternative to KNN in situations with large training data where we desire to generate large neighborhoods. 
 
Uniquely, the storage size of the IQNN r-tree and time required for prediction is not tied to the training set size. The aggregation within iterative-quantile bins provides an option for scaling a KNN-style method to cases with truly "big $n$" data scenarios if sufficient computational infrastructure is available at the time of model fitting to create bins. The iterative quantile binning algorithm could be implemented using a database query language to scale to situations with huge numbers of training observations because the iterative-quantile binning algorithm only requires tracking indeces in a sequence of quantile calculation, then calculating simple summary statistics over the resulting index groups. 

In Section~\ref{accuracy} that misclassification error rates for IQNN were quite comparable to that of the KNN and AKNN models. Notably, the misclassification error rates for all IQNN classifier models in our empirical analysis of tuned models was consistently within a few percent above or below the KNN accuracy.

%----------------------------------------------------------------------------
\begin{appendix}
%-------------------------------------------
\section{Appendix A: Timing Simulations}  
\label{timingAppend}

<<sim_times_tables, eval=F, include=F>>=
sim_times_tables <- sim_times_agg %>%
  select(n, k, d, iqnn_fittime,knn_cover_fittime,knn_kd_fittime,
        iqnn_predtime, knn_brute_predtime, knn_cover_predtime, knn_kd_predtime) %>%
  arrange(n,k,d)
print(xtable(sim_times_tables,digits=c(0,0,0,0,6,6,6,6,6,6,6)), include.rownames=FALSE)
@

\renewcommand{\arraystretch}{0.65}% Tighter
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{rrr|rrr|rrrr}
  \hline
\multicolumn{3}{l|}{Parameters} & \multicolumn{3}{l|}{Pre-processing Time (minutes)} & \multicolumn{4}{l}{Prediction Time (minutes)} \\   
n & $k$ & $\delta$ & IQNN & cover & kd & IQNN & KNN & cover & kd \\ 
  \hline
  16 & 1 & 4 & 0.000057 & 0.000028 & 0.000008 & 0.001213 & 0.000014 & 0.000103 & 0.000103 \\ 
  16 & 4 & 2 & 0.000050 & 0.000037 & 0.000012 & 0.001289 & 0.000019 & 0.000117 & 0.000123 \\ 
  64 & 1 & 8 & 0.000145 & 0.000030 & 0.000010 & 0.001279 & 0.000020 & 0.000112 & 0.000110 \\ 
  64 & 4 & 4 & 0.000081 & 0.000038 & 0.000017 & 0.001298 & 0.000023 & 0.000118 & 0.000120 \\ 
  64 & 16 & 2 & 0.000058 & 0.000061 & 0.000026 & 0.001294 & 0.000032 & 0.000130 & 0.000120 \\ 
  256 & 1 & 16 & 0.000460 & 0.000040 & 0.000012 & 0.001294 & 0.000035 & 0.000110 & 0.000114 \\ 
  256 & 4 & 8 & 0.000229 & 0.000057 & 0.000017 & 0.001322 & 0.000034 & 0.000124 & 0.000116 \\ 
  256 & 16 & 4 & 0.000124 & 0.000078 & 0.000036 & 0.001311 & 0.000057 & 0.000123 & 0.000133 \\ 
  256 & 64 & 2 & 0.000087 & 0.000202 & 0.000087 & 0.001311 & 0.000142 & 0.000156 & 0.000149 \\ 
  1024 & 1 & 32 & 0.002319 & 0.000070 & 0.000015 & 0.001283 & 0.000079 & 0.000120 & 0.000121 \\ 
  1024 & 4 & 16 & 0.001039 & 0.000081 & 0.000021 & 0.001296 & 0.000099 & 0.000120 & 0.000111 \\ 
  1024 & 16 & 8 & 0.000553 & 0.000123 & 0.000044 & 0.001298 & 0.000114 & 0.000128 & 0.000120 \\ 
  1024 & 64 & 4 & 0.000326 & 0.000297 & 0.000109 & 0.001316 & 0.000259 & 0.000154 & 0.000159 \\ 
  1024 & 256 & 2 & 0.000247 & 0.001283 & 0.000438 & 0.001331 & 0.001096 & 0.000212 & 0.000223 \\ 
  4096 & 1 & 64 & 0.014388 & 0.000146 & 0.000032 & 0.001268 & 0.000284 & 0.000113 & 0.000117 \\ 
  4096 & 4 & 32 & 0.006602 & 0.000165 & 0.000042 & 0.001287 & 0.000294 & 0.000127 & 0.000117 \\ 
  4096 & 16 & 16 & 0.003252 & 0.000226 & 0.000063 & 0.001286 & 0.000330 & 0.000125 & 0.000139 \\ 
  4096 & 64 & 8 & 0.001827 & 0.000468 & 0.000152 & 0.001294 & 0.000520 & 0.000165 & 0.000156 \\ 
  4096 & 256 & 4 & 0.001164 & 0.001815 & 0.000543 & 0.001304 & 0.001969 & 0.000225 & 0.000228 \\ 
  4096 & 1024 & 2 & 0.000816 & 0.012749 & 0.003413 & 0.001316 & 0.013068 & 0.000567 & 0.000584 \\ 
  16384 & 1 & 128 & 0.111621 & 0.000466 & 0.000122 & 0.001206 & 0.001073 & 0.000102 & 0.000112 \\ 
  16384 & 4 & 64 & 0.048643 & 0.000483 & 0.000135 & 0.001229 & 0.001084 & 0.000122 & 0.000124 \\ 
  16384 & 16 & 32 & 0.022962 & 0.000576 & 0.000154 & 0.001242 & 0.001141 & 0.000124 & 0.000129 \\ 
  16384 & 64 & 16 & 0.012068 & 0.000913 & 0.000260 & 0.001289 & 0.001393 & 0.000155 & 0.000149 \\ 
  16384 & 256 & 8 & 0.006921 & 0.002773 & 0.000760 & 0.001308 & 0.003451 & 0.000226 & 0.000232 \\ 
  16384 & 1024 & 4 & 0.004244 & 0.018174 & 0.004600 & 0.001311 & 0.022881 & 0.000602 & 0.000636 \\ 
  16384 & 4096 & 2 & 0.002915 & 0.171400 & 0.042028 & 0.001296 & 0.204892 & 0.001689 & 0.002087 \\ 
  65536 & 4 & 128 & 0.406102 & 0.001919 & 0.000603 & 0.001146 & 0.004226 & 0.000103 & 0.000106 \\ 
  65536 & 16 & 64 & 0.176751 & 0.001958 & 0.000653 & 0.001206 & 0.004240 & 0.000117 & 0.000131 \\ 
  65536 & 64 & 32 & 0.087728 & 0.002432 & 0.000785 & 0.001175 & 0.004580 & 0.000144 & 0.000141 \\ 
  65536 & 256 & 16 & 0.048294 & 0.004817 & 0.001376 & 0.001213 & 0.007136 & 0.000260 & 0.000280 \\ 
  65536 & 1024 & 8 & 0.025948 & 0.025055 & 0.005823 & 0.001234 & 0.034723 & 0.000717 & 0.000793 \\ 
  65536 & 4096 & 4 & 0.015947 & 0.246432 & 0.054589 & 0.001247 & 0.355684 & 0.001843 & 0.001896 \\ 
  65536 & 16384 & 2 & 0.011769 & 2.726151 & 0.633031 & 0.001275 & 3.301747 & 0.006239 & 0.007206 \\ 
  262144 & 16 & 128 & 1.577832 & 0.008750 & 0.002914 & 0.001166 & 0.016989 & 0.000166 & 0.000143 \\ 
  262144 & 64 & 64 & 0.697907 & 0.009569 & 0.003276 & 0.001162 & 0.017434 & 0.000213 & 0.000262 \\ 
  262144 & 256 & 32 & 0.339445 & 0.012685 & 0.004424 & 0.001138 & 0.020513 & 0.000301 & 0.000334 \\ 
  262144 & 1024 & 16 & 0.180536 & 0.038859 & 0.011281 & 0.001192 & 0.056422 & 0.001010 & 0.000748 \\ 
  262144 & 4096 & 8 & 0.102174 & 0.334415 & 0.073338 & 0.001202 & 0.518069 & 0.002273 & 0.002231 \\ 
  262144 & 16384 & 4 & 0.066457 & 3.962837 & 0.837682 & 0.001373 & 5.744885 & 0.007897 & 0.008069 \\ 
  1048576 & 64 & 128 & 6.359297 & 0.041474 & 0.015857 & 0.001130 & 0.070609 & 0.000781 & 0.000337 \\ 
  1048576 & 256 & 64 & 2.773028 & 0.045912 & 0.016926 & 0.001121 & 0.074784 & 0.001376 & 0.000580 \\ 
  1048576 & 1024 & 32 & 1.337717 & 0.080840 & 0.025407 & 0.001136 & 0.120404 & 0.002076 & 0.001233 \\ 
  1048576 & 4096 & 16 & 0.712297 & 0.455469 & 0.094837 & 0.001143 & 0.721843 & 0.003800 & 0.003342 \\ 
  1048576 & 16384 & 8 & 0.423727 & 5.236268 & 0.954038 & 0.001210 & 8.229757 & 0.010522 & 0.010947 \\ 
   \hline
\end{tabular}
\caption{\small Average pre-processing and prediction times from 500 simulations per parameterization.}
\label{tab:sim_times_table}
\end{table}
\normalsize

%-------------------------------------------
\section{Appendix B: Empirical Data Sets}  
\label{dataAppend}

\renewcommand{\arraystretch}{0.75}
\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{lrl}
\textbf{Classification} & & \\
\hline 
Data & n & Web Address (Accessed: 10/25/17) \\ 
\hline 
  iris & 150 & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/iris} \\ 
  wpbc & 198 & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin} \\ 
  pima & 768 & \url{archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes} \\
  yeast & 1484 & \url{archive.ics.uci.edu/ml/machine-learning-databases/yeast} \\ 
  abalone & 4174 & \url{archive.ics.uci.edu/ml/machine-learning-databases/abalone} \\ 
  waveform & 5000 & \url{archive.ics.uci.edu/ml/machine-learning-databases/waveform} \\ 
  optdigits & 5620 & \url{sci2s.ugr.es/keel/dataset.php?cod=199} \\ 
  satimage & 6435 & \url{sci2s.ugr.es/keel/dataset.php?cod=71} \\ 
  marketing & 8494 & \url{sci2s.ugr.es/keel/dataset.php?cod=163} \\
  seizure & 11500 & \url{archive.ics.uci.edu/ml/machine-learning-databases/00388} \\ 
\hline 
 & & \\
\textbf{Regression} & & \\ 
\hline 
Data & n & Web Address (Accessed: 10/25/17) \\ 
\hline 
  wpbc & 198 &  \url{archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin} \\ 
  wankara & 321 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=41} \\ 
  laser & 993 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=47} \\ 
  treasury & 1049 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=42} \\
  quake & 2178 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=75} \\ 
  skillcraft & 3395 &  \url{archive.ics.uci.edu/ml/machine-learning-databases/00272} \\
  anacalt & 4052 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=159} \\
  puma & 8192 &  \url{sci2s.ugr.es/keel/category.php?cat=reg?cod=1291} \\
  air quality & 9357 &  \url{archive.ics.uci.edu/ml/machine-learning-databases/00360} \\
  ccpp & 9568 &  \url{archive.ics.uci.edu/ml/machine-learning-databases/00294} \\
   \hline
\end{tabular}
\caption{\small Sources for data sets used for testing predictive accuracy for models in Section~\ref{accuracy}.}
\label{tab:data_repos}
\end{table}
\normalsize

%-------------------------------------------

\section{Appendix C: Model Parameters and Performance}  
\label{accuracyAppend}

<<data_tables, eval=F, include=F>>=
# create basis for class data table
organized_class_data <- arrange(unique(results_class[,1:2]),obs)
print(xtable(organized_class_data), include.rownames=FALSE)

# create class data performance table
all_sets <- c("iris","wpbc","pima","yeast","abalone","waveform","optdigits","satimage","marketing","seizure")
class_tune_table <- data.frame(data=all_sets, 
                               n=sapply(tuned_param_list, function(x) x$n),
                               nbins=paste0("(",sapply(tuned_param_list, function(x) paste(x$nbins,collapse=",")),")"),
                               k=sapply(tuned_param_list, function(x) x$k) )
organized_class_param <- arrange(class_tune_table,n)

results_class_wide <- results_class%>%
  mutate(avg_misclass = 1-avg_cv_accuracy) %>%
  select(data_name,type,avg_misclass) %>%
  spread(key=type,value=avg_misclass)

full_class <- inner_join(organized_class_param,results_class_wide, by=c(data="data_name"))
print(xtable(full_class,digits=4), include.rownames=FALSE)

# create basis for reg data table
organized_reg_data <- arrange(unique(results_reg[,1:2]),obs)
print(xtable(organized_reg_data), include.rownames=FALSE)

# create reg data performance table
all_reg_sets <- c("wpbc","wankara","laser","treasury","quake","skillcraft","anacalt","puma","air_quality","ccpp")
reg_tune_table <- data.frame(data=all_reg_sets, 
                               n=sapply(tuned_reg_param_list, function(x) x$n),
                               nbins=paste0("(",sapply(tuned_reg_param_list, function(x) paste(x$nbins,collapse=",")),")"),
                               k=sapply(tuned_reg_param_list, function(x) x$k) )
organized_reg_param <- arrange(reg_tune_table,n)

results_reg_wide <- results_reg%>%
  select(data_name,type,rmse) %>%
  spread(key=type,value=rmse)

full_reg <- inner_join(organized_reg_param,results_reg_wide, by=c(data="data_name"))
print(xtable(full_reg,digits=4), include.rownames=FALSE)
@


\begin{table}[H]
\footnotesize
\centering
\begin{tabular}{lr|cr|rrrr} 
\textbf{Classification}\\
\hline
\multicolumn{2}{l|}{ } & \multicolumn{2}{c|}{Tuned Parameters} & \multicolumn{4}{c}{Average Prop. Misclassified} \\
Data & n & $(\delta_1,\delta_2)$ & $k$ & IQNN & KNN & cover &  kd  \\
\hline  
 iris &   150 & (3,3) &     7 & 0.0543 & 0.0363 & 0.0363 & 0.0363 \\ 
  wpbc &   198 & (3,2) &    45 & 0.2388 & 0.2374 & 0.2374 & 0.2374 \\ 
  pima &   768 & (6,5) &   115 & 0.2495 & 0.2465 & 0.2464 & 0.2465 \\ 
  yeast &  1484 & (10,9) &   111 & 0.5541 & 0.5546 & 0.5544 & 0.5549 \\ 
  abalone &  4174 & (6,6) &   313 & 0.4564 & 0.4595 & 0.4595 & 0.4595 \\ 
  waveform &  5000 & (6,5) &   750 & 0.3895 & 0.3900 & 0.3900 & 0.3900 \\ 
  optdigits &  5620 & (8,7) &    70 & 0.5854 & 0.5835 & 0.5850 & 0.5866 \\ 
  satimage &  6435 & (15,15) &    37 & 0.2115 & 0.2039 & 0.2041 & 0.2039 \\ 
  marketing &  8494 & (10,9) &   137 & 0.3802 & 0.3736 & 0.3779 & 0.3841 \\ 
  seizure & 11500 & (14,13) &    94 & 0.6146 & 0.6013 & 0.6013 & 0.6012 \\ 
    \hline
    \\
    \textbf{Regression} \\
    \hline
\multicolumn{2}{l|}{ } & \multicolumn{2}{c|}{Tuned Parameters} & \multicolumn{4}{c}{Average RMSE} \\
Data & n & $(\delta_1,\delta_2)$ & $k$ & IQNN & KNN & cover &  kd  \\
\hline
  wpbc &   198 & (3,2) &    30 & 0.9322 & 0.9129 & 0.9129 & 0.9129 \\ 
  wankara &   321 & (9,8) &     3 & 0.1664 & 0.1277 & 0.1277 & 0.1277 \\ 
  laser &   993 & (5,5) &    25 & 0.5694 & 0.5645 & 0.5643 & 0.5643 \\ 
  treasury &  1049 & (14,14) &     9 & 0.0953 & 0.0772 & 0.0772 & 0.0772 \\ 
  quake &  2178 & (2,2) &   490 & 0.9989 & 0.9990 & 0.9990 & 0.9990 \\ 
  skillcraft &  3395 & (13,12) &    48 & 0.6325 & 0.6163 & 0.6163 & 0.6163 \\ 
  anacalt &  4052 & (30,30) &     8 & 0.1551 & 0.1445 & 0.1477 & 0.1445 \\ 
  puma &  8192 & (26,26) &    33 & 0.2666 & 0.2504 & 0.2504 & 0.2504 \\ 
  air\_quality &  9357 & (9,9) &   117 & 0.7188 & 0.7145 & 0.7149 & 0.7231 \\ 
  ccpp &  9568 & (38,38) &    13 & 0.2513 & 0.2519 & 0.2519 & 0.2519 \\ 
\hline
\end{tabular}
\caption{\small Parameters selected through tuning and average model error rates from 100 initialization of ten-fold cross-validation. Classification models evaluated using misclassification error proportions. Regression models evaluated with root mean-squared error.}
\label{tab:tuning_reg_class}
\end{table}
% 
% \begin{table}[H]
% \small
% \centering
% \begin{tabular}{|lr|cr|rrrr|} \hline
% Data & n & tuned-$(\delta_1,\delta_2)$ & tuned-$k$ & IQNN & KNN & cover &  kd  \\
%   \hline
%   wpbc &   198 & (3,2) &    30 & 0.9322 & 0.9129 & 0.9129 & 0.9129 \\ 
%   wankara &   321 & (9,8) &     3 & 0.1664 & 0.1277 & 0.1277 & 0.1277 \\ 
%   laser &   993 & (5,5) &    25 & 0.5694 & 0.5645 & 0.5643 & 0.5643 \\ 
%   treasury &  1049 & (14,14) &     9 & 0.0953 & 0.0772 & 0.0772 & 0.0772 \\ 
%   quake &  2178 & (2,2) &   490 & 0.9989 & 0.9990 & 0.9990 & 0.9990 \\ 
%   skillcraft &  3395 & (13,12) &    48 & 0.6325 & 0.6163 & 0.6163 & 0.6163 \\ 
%   anacalt &  4052 & (30,30) &     8 & 0.1551 & 0.1445 & 0.1477 & 0.1445 \\ 
%   puma &  8192 & (26,26) &    33 & 0.2666 & 0.2504 & 0.2504 & 0.2504 \\ 
%   air\_quality &  9357 & (9,9) &   117 & 0.7188 & 0.7145 & 0.7149 & 0.7231 \\ 
%   ccpp &  9568 & (38,38) &    13 & 0.2513 & 0.2519 & 0.2519 & 0.2519 \\ 
% \hline
% \end{tabular}
% \caption{Tuned parameters for regression models and average RMSE from 100 initialization of ten-fold cross-validation}
% \label{tab:tuning_class}
% \end{table}

%----------------------------------------------------------------------------
\end{appendix}

\newpage

\bibliographystyle{asa}

\spacingset{1} % reference spacing

\bibliography{references}

\end{document}
